{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhihaohaoran/GPGS/blob/main/icml_3dgs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhoWJ5zm0pBw",
        "outputId": "b62812c3-2036-49fd-bf9a-0cf5de798d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ninja-build\n",
        "!pip install cmake\n",
        "!git clone --recursive https://github.com/NVlabs/tiny-cuda-nn.git\n",
        "%cd tiny-cuda-nn\n",
        "!cmake . -B build -DCMAKE_BUILD_TYPE=RelWithDebInfo -DTCNN_BUILD_BENCHMARK=OFF -DTCNN_BUILD_PYTHON=ON\n",
        "!cmake --build build --config RelWithDebInfo -j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ-kPWL71NCy",
        "outputId": "86f308d0-f8ab-417c-8b5a-5e69985330a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,681 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,770 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [112 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [38.5 kB]\n",
            "Fetched 24.4 MB in 5s (4,620 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ninja-build is already the newest version (1.10.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (3.31.6)\n",
            "Cloning into 'tiny-cuda-nn'...\n",
            "remote: Enumerating objects: 4164, done.\u001b[K\n",
            "remote: Counting objects: 100% (1160/1160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 4164 (delta 1046), reused 1001 (delta 1001), pack-reused 3004 (from 4)\u001b[K\n",
            "Receiving objects: 100% (4164/4164), 19.66 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (2645/2645), done.\n",
            "Submodule 'dependencies/cutlass' (https://github.com/NVIDIA/cutlass) registered for path 'dependencies/cutlass'\n",
            "Submodule 'dependencies/fmt' (https://github.com/fmtlib/fmt) registered for path 'dependencies/fmt'\n",
            "Cloning into '/content/tiny-cuda-nn/tiny-cuda-nn/dependencies/cutlass'...\n",
            "remote: Enumerating objects: 32604, done.        \n",
            "remote: Counting objects: 100% (526/526), done.        \n",
            "remote: Compressing objects: 100% (276/276), done.        \n",
            "remote: Total 32604 (delta 355), reused 250 (delta 250), pack-reused 32078 (from 3)        \n",
            "Receiving objects: 100% (32604/32604), 49.27 MiB | 16.88 MiB/s, done.\n",
            "Resolving deltas: 100% (24588/24588), done.\n",
            "Cloning into '/content/tiny-cuda-nn/tiny-cuda-nn/dependencies/fmt'...\n",
            "remote: Enumerating objects: 39196, done.        \n",
            "remote: Counting objects: 100% (3146/3146), done.        \n",
            "remote: Compressing objects: 100% (201/201), done.        \n",
            "remote: Total 39196 (delta 3028), reused 2950 (delta 2943), pack-reused 36050 (from 4)        \n",
            "Receiving objects: 100% (39196/39196), 15.96 MiB | 13.91 MiB/s, done.\n",
            "Resolving deltas: 100% (26486/26486), done.\n",
            "Submodule path 'dependencies/cutlass': checked out '1eb6355182a5124639ce9d3ff165732a94ed9a70'\n",
            "Submodule path 'dependencies/fmt': checked out 'b0c8263cb26ea178d3a5df1b984e1a61ef578950'\n",
            "/content/tiny-cuda-nn/tiny-cuda-nn\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Obtained CUDA architectures automatically from installed GPUs\n",
            "-- Targeting CUDA architectures: 80\n",
            "-- Module support is disabled.\n",
            "-- Version: 9.1.1\n",
            "-- Build type: RelWithDebInfo\n",
            "-- Configuring done (3.3s)\n",
            "-- Generating done (0.0s)\n",
            "\u001b[33mCMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    TCNN_BUILD_PYTHON\n",
            "\n",
            "\u001b[0m\n",
            "-- Build files have been written to: /content/tiny-cuda-nn/tiny-cuda-nn/build\n",
            "[  5%] \u001b[32mBuilding CXX object dependencies/fmt/CMakeFiles/fmt.dir/src/format.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object dependencies/fmt/CMakeFiles/fmt.dir/src/os.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX static library libfmt.a\u001b[0m\n",
            "[ 17%] Built target fmt\n",
            "[ 29%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/cpp_api.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/common_host.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/cutlass_mlp.cu.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/encoding.cu.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/loss.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/network.cu.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/object.cu.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/optimizer.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/reduce_sum.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/fully_fused_mlp.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX static library libtiny-cuda-nn.a\u001b[0m\n",
            "[ 82%] Built target tiny-cuda-nn\n",
            "[ 94%] \u001b[32mBuilding CUDA object samples/CMakeFiles/mlp_learning_an_image.dir/mlp_learning_an_image.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object samples/CMakeFiles/mlp_learning_an_image.dir/__/dependencies/stbi/stbi_wrapper.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../mlp_learning_an_image\u001b[0m\n",
            "[100%] Built target mlp_learning_an_image\n",
            "[Errno 2] No such file or directory: 'bindings/py'\n",
            "/content/tiny-cuda-nn/tiny-cuda-nn\n",
            "Obtaining file:///content/tiny-cuda-nn/tiny-cuda-nn\n",
            "\u001b[31mERROR: file:///content/tiny-cuda-nn/tiny-cuda-nn does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/tiny-cuda-nn/bindings/torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnW6E2rB4Z7y",
        "outputId": "ee083a3d-84c8-4089-9683-8255e19d0d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/  \u001b[01;34mdependencies\u001b[0m/  setup.py  \u001b[01;34msrc\u001b[0m/  \u001b[01;34mtinycudann\u001b[0m/  \u001b[01;34mtinycudann.egg-info\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ⚠️ Step 1：清空当前错误的版本（确保不会冲突）\n",
        "%cd /content\n",
        "!rm -rf tiny-cuda-nn\n",
        "\n",
        "# ✅ Step 2：正确 clone tiny-cuda-nn（含所有依赖）\n",
        "!git clone --recursive https://github.com/NVlabs/tiny-cuda-nn.git\n",
        "%cd tiny-cuda-nn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzgXBatQ47j5",
        "outputId": "92ade4ae-c714-4670-d138-98eb581debd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'tiny-cuda-nn'...\n",
            "remote: Enumerating objects: 4164, done.\u001b[K\n",
            "remote: Counting objects: 100% (1162/1162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 4164 (delta 1046), reused 1001 (delta 1001), pack-reused 3002 (from 4)\u001b[K\n",
            "Receiving objects: 100% (4164/4164), 19.66 MiB | 16.75 MiB/s, done.\n",
            "Resolving deltas: 100% (2645/2645), done.\n",
            "Submodule 'dependencies/cutlass' (https://github.com/NVIDIA/cutlass) registered for path 'dependencies/cutlass'\n",
            "Submodule 'dependencies/fmt' (https://github.com/fmtlib/fmt) registered for path 'dependencies/fmt'\n",
            "Cloning into '/content/tiny-cuda-nn/dependencies/cutlass'...\n",
            "remote: Enumerating objects: 32604, done.        \n",
            "remote: Counting objects: 100% (522/522), done.        \n",
            "remote: Compressing objects: 100% (278/278), done.        \n",
            "remote: Total 32604 (delta 351), reused 244 (delta 244), pack-reused 32082 (from 3)        \n",
            "Receiving objects: 100% (32604/32604), 49.06 MiB | 16.66 MiB/s, done.\n",
            "Resolving deltas: 100% (24585/24585), done.\n",
            "Cloning into '/content/tiny-cuda-nn/dependencies/fmt'...\n",
            "remote: Enumerating objects: 39196, done.        \n",
            "remote: Counting objects: 100% (3188/3188), done.        \n",
            "remote: Compressing objects: 100% (199/199), done.        \n",
            "remote: Total 39196 (delta 3072), reused 2994 (delta 2987), pack-reused 36008 (from 4)        \n",
            "Receiving objects: 100% (39196/39196), 16.39 MiB | 15.38 MiB/s, done.\n",
            "Resolving deltas: 100% (26485/26485), done.\n",
            "Submodule path 'dependencies/cutlass': checked out '1eb6355182a5124639ce9d3ff165732a94ed9a70'\n",
            "Submodule path 'dependencies/fmt': checked out 'b0c8263cb26ea178d3a5df1b984e1a61ef578950'\n",
            "/content/tiny-cuda-nn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 3：重新构建\n",
        "!cmake . -B build -DCMAKE_BUILD_TYPE=RelWithDebInfo -DTCNN_BUILD_PYTHON=ON\n",
        "!cmake --build build --config RelWithDebInfo -j\n",
        "\n",
        "# ✅ Step 4：进入 Python binding 目录并安装\n",
        "%cd bindings/torch\n",
        "!pip install .\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPrzBpw_1RK-",
        "outputId": "e0d73cd6-bddd-4512-a315-b59dcd9a5db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Obtained CUDA architectures automatically from installed GPUs\n",
            "-- Targeting CUDA architectures: 80\n",
            "-- Module support is disabled.\n",
            "-- Version: 9.1.1\n",
            "-- Build type: RelWithDebInfo\n",
            "-- Configuring done (3.5s)\n",
            "-- Generating done (0.0s)\n",
            "\u001b[33mCMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    TCNN_BUILD_PYTHON\n",
            "\n",
            "\u001b[0m\n",
            "-- Build files have been written to: /content/tiny-cuda-nn/build\n",
            "[  5%] \u001b[32mBuilding CXX object dependencies/fmt/CMakeFiles/fmt.dir/src/format.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object dependencies/fmt/CMakeFiles/fmt.dir/src/os.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX static library libfmt.a\u001b[0m\n",
            "[ 15%] Built target fmt\n",
            "[ 20%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/common_host.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/cutlass_mlp.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/cpp_api.cu.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/loss.cu.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/encoding.cu.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/network.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/object.cu.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/reduce_sum.cu.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/optimizer.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CUDA object CMakeFiles/tiny-cuda-nn.dir/src/fully_fused_mlp.cu.o\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tinycudann as tcnn\n",
        "print(tcnn.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "KTcjbhVVc806",
        "outputId": "a606aeaa-c920-4e6d-bf99-e32e8b7525de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tinycudann'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ee067cb637f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtinycudann\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tinycudann'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soc_OMMQ09jc"
      },
      "outputs": [],
      "source": [
        "!pip install -q plyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5bQOgjh1ENB",
        "outputId": "14334169-5195-4f6e-acff-26da5a7abb83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diff_gaussian_rasterization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for simple_knn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fused_ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q /content/drive/MyDrive/3dgs/gaussian-splatting/submodules/diff-gaussian-rasterization\n",
        "!pip install -q /content/drive/MyDrive/3dgs/gaussian-splatting/submodules/simple-knn\n",
        "!pip install -q /content/drive/MyDrive/3dgs/gaussian-splatting/submodules/fused-ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlg5co4EAevr",
        "outputId": "250e6faf-49b0-440c-e7bf-32afede06a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/3dgs\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/3dgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzln4Sum_6g6",
        "outputId": "775c0252-b1eb-4dcf-ceb3-13a4912ec9e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gaussian-splatting'...\n",
            "remote: Enumerating objects: 859, done.\u001b[K\n",
            "remote: Total 859 (delta 0), reused 0 (delta 0), pack-reused 859 (from 1)\u001b[K\n",
            "Receiving objects: 100% (859/859), 78.63 MiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (490/490), done.\n",
            "Updating files: 100% (64/64), done.\n",
            "Submodule 'SIBR_viewers' (https://gitlab.inria.fr/sibr/sibr_core.git) registered for path 'SIBR_viewers'\n",
            "Submodule 'submodules/diff-gaussian-rasterization' (https://github.com/graphdeco-inria/diff-gaussian-rasterization.git) registered for path 'submodules/diff-gaussian-rasterization'\n",
            "Submodule 'submodules/fused-ssim' (https://github.com/rahul-goel/fused-ssim.git) registered for path 'submodules/fused-ssim'\n",
            "Submodule 'submodules/simple-knn' (https://gitlab.inria.fr/bkerbl/simple-knn.git) registered for path 'submodules/simple-knn'\n",
            "Cloning into '/content/drive/MyDrive/3dgs/gaussian-splatting/SIBR_viewers'...\n",
            "remote: Enumerating objects: 3293, done.        \n",
            "remote: Counting objects: 100% (322/322), done.        \n",
            "remote: Compressing objects: 100% (174/174), done.        \n",
            "remote: Total 3293 (delta 171), reused 280 (delta 148), pack-reused 2971 (from 1)        \n",
            "Receiving objects: 100% (3293/3293), 9.98 MiB | 8.99 MiB/s, done.\n",
            "Resolving deltas: 100% (2039/2039), done.\n",
            "Cloning into '/content/drive/MyDrive/3dgs/gaussian-splatting/submodules/diff-gaussian-rasterization'...\n",
            "remote: Enumerating objects: 329, done.        \n",
            "remote: Counting objects: 100% (189/189), done.        \n",
            "remote: Compressing objects: 100% (53/53), done.        \n",
            "remote: Total 329 (delta 161), reused 136 (delta 136), pack-reused 140 (from 1)        \n",
            "Receiving objects: 100% (329/329), 126.27 KiB | 1.80 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n",
            "Cloning into '/content/drive/MyDrive/3dgs/gaussian-splatting/submodules/fused-ssim'...\n",
            "remote: Enumerating objects: 99, done.        \n",
            "remote: Counting objects: 100% (99/99), done.        \n",
            "remote: Compressing objects: 100% (74/74), done.        \n",
            "remote: Total 99 (delta 49), reused 56 (delta 21), pack-reused 0 (from 0)        \n",
            "Receiving objects: 100% (99/99), 3.66 MiB | 13.64 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "Cloning into '/content/drive/MyDrive/3dgs/gaussian-splatting/submodules/simple-knn'...\n",
            "remote: Enumerating objects: 37, done.        \n",
            "remote: Counting objects: 100% (37/37), done.        \n",
            "remote: Compressing objects: 100% (34/34), done.        \n",
            "remote: Total 37 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)        \n",
            "Receiving objects: 100% (37/37), 9.41 KiB | 1.34 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Submodule path 'SIBR_viewers': checked out 'd8856f60c5384cc1975439193bb627d77d917d77'\n",
            "Submodule path 'submodules/diff-gaussian-rasterization': checked out '9c5c2028f6fbee2be239bc4c9421ff894fe4fbe0'\n",
            "Submodule 'third_party/glm' (https://github.com/g-truc/glm.git) registered for path 'submodules/diff-gaussian-rasterization/third_party/glm'\n",
            "Cloning into '/content/drive/MyDrive/3dgs/gaussian-splatting/submodules/diff-gaussian-rasterization/third_party/glm'...\n",
            "remote: Enumerating objects: 59639, done.        \n",
            "remote: Counting objects: 100% (21/21), done.        \n",
            "remote: Compressing objects: 100% (21/21), done.        \n",
            "remote: Total 59639 (delta 9), reused 0 (delta 0), pack-reused 59618 (from 3)        \n",
            "Receiving objects: 100% (59639/59639), 71.23 MiB | 18.61 MiB/s, done.\n",
            "Resolving deltas: 100% (45218/45218), done.\n",
            "Submodule path 'submodules/diff-gaussian-rasterization/third_party/glm': checked out '5c46b9c07008ae65cb81ab79cd677ecc1934b903'\n",
            "Submodule path 'submodules/fused-ssim': checked out '1272e21a282342e89537159e4bad508b19b34157'\n",
            "Submodule path 'submodules/simple-knn': checked out '86710c2d4b46680c02301765dd79e465819c8f19'\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIRMLEFo_-aq",
        "outputId": "d0ef4a4e-ad05-4ef1-f33f-8dccb8f51635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/3dgs/gaussian-splatting\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/3dgs/gaussian-splatting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nerfstudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fu7ARLr0kJDE",
        "outputId": "ab84779d-a05a-4d08-bc1b-63ff041766ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nerfstudio\n",
            "  Downloading nerfstudio-1.1.5-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting appdirs>=1.4 (from nerfstudio)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting av>=9.2.0 (from nerfstudio)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting comet-ml>=3.33.8 (from nerfstudio)\n",
            "  Downloading comet_ml-3.49.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: cryptography>=38 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (43.0.3)\n",
            "Collecting tyro>=0.6.6 (from nerfstudio)\n",
            "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: gdown>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (5.2.0)\n",
            "Collecting ninja>=1.10 (from nerfstudio)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (3.12.1)\n",
            "Requirement already satisfied: imageio>=2.21.1 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (2.37.0)\n",
            "Requirement already satisfied: ipywidgets>=7.6 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (7.7.1)\n",
            "Collecting jaxtyping>=0.2.15 (from nerfstudio)\n",
            "  Downloading jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting jupyterlab>=3.3.4 (from nerfstudio)\n",
            "  Downloading jupyterlab-4.3.6-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (3.10.0)\n",
            "Collecting mediapy>=1.1.0 (from nerfstudio)\n",
            "  Downloading mediapy-1.2.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: msgpack>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (1.1.0)\n",
            "Collecting msgpack-numpy>=0.4.8 (from nerfstudio)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting nerfacc==0.5.2 (from nerfstudio)\n",
            "  Downloading nerfacc-0.5.2-py3-none-any.whl.metadata (877 bytes)\n",
            "Collecting open3d>=0.16.0 (from nerfstudio)\n",
            "  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from nerfstudio)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (11.1.0)\n",
            "Requirement already satisfied: plotly>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (5.24.1)\n",
            "Collecting protobuf!=3.20.0,<=3.20.3 (from nerfstudio)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pyngrok>=5.1.0 (from nerfstudio)\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting python-socketio>=5.7.1 (from nerfstudio)\n",
            "  Downloading python_socketio-5.12.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyquaternion>=0.9.9 (from nerfstudio)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (2.32.3)\n",
            "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (13.9.4)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (0.25.2)\n",
            "Collecting splines==0.3.0 (from nerfstudio)\n",
            "  Downloading splines-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tensorboard>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (2.18.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (0.21.0+cu124)\n",
            "Collecting torchmetrics>=1.0.1 (from torchmetrics[image]>=1.0.1->nerfstudio)\n",
            "  Downloading torchmetrics-1.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (4.12.2)\n",
            "Collecting viser==0.2.7 (from nerfstudio)\n",
            "  Downloading viser-0.2.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting nuscenes-devkit>=1.1.1 (from nerfstudio)\n",
            "  Downloading nuscenes_devkit-1.1.11-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wandb>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (0.19.8)\n",
            "Collecting xatlas (from nerfstudio)\n",
            "  Downloading xatlas-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting trimesh>=3.20.2 (from nerfstudio)\n",
            "  Downloading trimesh-4.6.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting timm==0.6.7 (from nerfstudio)\n",
            "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting gsplat==1.4.0 (from nerfstudio)\n",
            "  Downloading gsplat-1.4.0-py3-none-any.whl.metadata (957 bytes)\n",
            "Collecting pytorch-msssim (from nerfstudio)\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pathos (from nerfstudio)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nerfstudio) (24.2)\n",
            "Collecting fpsample (from nerfstudio)\n",
            "  Downloading fpsample-0.3.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting tensorly (from nerfstudio)\n",
            "  Downloading tensorly-0.9.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting rawpy>=0.18.1 (from nerfstudio)\n",
            "  Downloading rawpy-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting pymeshlab>=2022.2.post2 (from nerfstudio)\n",
            "  Downloading pymeshlab-2023.12.post3-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gsplat==1.4.0->nerfstudio) (1.26.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from viser==0.2.7->nerfstudio) (14.2)\n",
            "Collecting msgspec>=0.18.6 (from viser==0.2.7->nerfstudio)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from viser==0.2.7->nerfstudio) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from viser==0.2.7->nerfstudio) (4.67.1)\n",
            "Collecting nodeenv>=1.8.0 (from viser==0.2.7->nerfstudio)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: psutil>=5.9.5 in /usr/local/lib/python3.11/dist-packages (from viser==0.2.7->nerfstudio) (5.9.5)\n",
            "Collecting yourdfpy>=0.0.53 (from viser==0.2.7->nerfstudio)\n",
            "  Downloading yourdfpy-0.0.57-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: plyfile>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from viser==0.2.7->nerfstudio) (1.1)\n",
            "Collecting pyliblzfse>=0.4.1 (from viser==0.2.7->nerfstudio)\n",
            "  Downloading pyliblzfse-0.4.1.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading dulwich-0.22.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (4.23.0)\n",
            "Collecting python-box<7.0.0 (from comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (1.0.0)\n",
            "Collecting semantic-version>=2.8.0 (from comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (2.22.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (3.20.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml>=3.33.8->nerfstudio) (1.17.2)\n",
            "Collecting wurlitzer>=1.0.2 (from comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=38->nerfstudio) (1.17.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.6.0->nerfstudio) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.6.0->nerfstudio) (3.17.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6->nerfstudio) (3.0.13)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.15->nerfstudio)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (75.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=3.3.4->nerfstudio) (6.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nerfstudio) (2.8.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit>=1.1.1->nerfstudio) (5.5.2)\n",
            "Collecting descartes (from nuscenes-devkit>=1.1.1->nerfstudio)\n",
            "  Downloading descartes-1.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting fire (from nuscenes-devkit>=1.1.1->nerfstudio)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of nuscenes-devkit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting nuscenes-devkit>=1.1.1 (from nerfstudio)\n",
            "  Downloading nuscenes_devkit-1.1.10-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting jupyter (from nuscenes-devkit>=1.1.1->nerfstudio)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting nuscenes-devkit>=1.1.1 (from nerfstudio)\n",
            "  Downloading nuscenes_devkit-1.1.9-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit>=1.1.1->nerfstudio) (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit>=1.1.1->nerfstudio) (1.6.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit>=1.1.1->nerfstudio) (2.0.7)\n",
            "Requirement already satisfied: pycocotools>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit>=1.1.1->nerfstudio) (2.0.8)\n",
            "Collecting dash>=2.6.0 (from open3d>=0.16.0->nerfstudio)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d>=0.16.0->nerfstudio) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d>=0.16.0->nerfstudio) (3.1.0)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d>=0.16.0->nerfstudio) (5.10.4)\n",
            "Collecting configargparse (from open3d>=0.16.0->nerfstudio)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=7.6 (from nerfstudio)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d>=0.16.0->nerfstudio)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d>=0.16.0->nerfstudio) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d>=0.16.0->nerfstudio) (6.0.2)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=7.6->nerfstudio)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=7.6->nerfstudio)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.7.0->nerfstudio) (9.0.0)\n",
            "Collecting bidict>=0.21.0 (from python-socketio>=5.7.1->nerfstudio)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting python-engineio>=4.11.0 (from python-socketio>=5.7.1->nerfstudio)\n",
            "  Downloading python_engineio-4.11.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nerfstudio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nerfstudio) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nerfstudio) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->nerfstudio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->nerfstudio) (2.18.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nerfstudio) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nerfstudio) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nerfstudio) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->nerfstudio) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->nerfstudio) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->nerfstudio) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->nerfstudio) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.13.0->nerfstudio) (0.7.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->nerfstudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->nerfstudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->nerfstudio) (1.3.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=1.0.1->torchmetrics[image]>=1.0.1->nerfstudio)\n",
            "  Downloading lightning_utilities-0.14.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]>=1.0.1->nerfstudio)\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.6.6->nerfstudio) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.6.6->nerfstudio)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.6.6->nerfstudio) (4.4.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (4.3.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (2.10.6)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.3->nerfstudio) (1.3.5)\n",
            "Collecting ppft>=1.7.6.9 (from pathos->nerfstudio)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos->nerfstudio)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos->nerfstudio)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos->nerfstudio)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=38->nerfstudio) (2.22)\n",
            "Collecting flask>=3.0.0 (from open3d>=0.16.0->nerfstudio)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d>=0.16.0->nerfstudio)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d>=0.16.0->nerfstudio)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d>=0.16.0->nerfstudio)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d>=0.16.0->nerfstudio)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d>=0.16.0->nerfstudio) (8.6.1)\n",
            "Collecting retrying (from dash>=2.6.0->open3d>=0.16.0->nerfstudio)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d>=0.16.0->nerfstudio) (1.6.0)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet-ml>=3.33.8->nerfstudio)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d>=0.16.0->nerfstudio) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d>=0.16.0->nerfstudio) (1.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.3->nerfstudio) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab>=3.3.4->nerfstudio) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab>=3.3.4->nerfstudio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab>=3.3.4->nerfstudio) (0.14.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio) (0.1.7)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio) (24.0.1)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyterlab>=3.3.4->nerfstudio) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml>=3.33.8->nerfstudio) (25.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml>=3.33.8->nerfstudio) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml>=3.33.8->nerfstudio) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml>=3.33.8->nerfstudio) (0.23.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (23.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (7.16.6)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.21.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.3.4->nerfstudio) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->nerfstudio) (0.1.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d>=0.16.0->nerfstudio) (2.21.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d>=0.16.0->nerfstudio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d>=0.16.0->nerfstudio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.3->nerfstudio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.3->nerfstudio) (2.27.2)\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.11.0->python-socketio>=5.7.1->nerfstudio)\n",
            "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nuscenes-devkit>=1.1.1->nerfstudio) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nuscenes-devkit>=1.1.1->nerfstudio) (3.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from yourdfpy>=0.0.53->viser==0.2.7->nerfstudio) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.6.0->nerfstudio) (2.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->nuscenes-devkit>=1.1.1->nerfstudio) (2.5.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->nuscenes-devkit>=1.1.1->nerfstudio) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->nuscenes-devkit>=1.1.1->nerfstudio) (6.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.6.0->nerfstudio) (1.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab>=3.3.4->nerfstudio) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (21.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.3->nerfstudio) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (0.8.4)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (1.5.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.6->nerfstudio) (0.2.13)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio>=5.7.1->nerfstudio)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting colorlog (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting manifold3d>=2.3.0 (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading manifold3d-3.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Collecting svg.path (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading svg.path-6.3-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pycollada (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading pycollada-0.9.tar.gz (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xxhash (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rtree (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting embreex (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting vhacdx (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading vhacdx-0.0.8.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting mapbox_earcut>=1.0.2 (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser==0.2.7->nerfstudio)\n",
            "  Downloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d>=0.16.0->nerfstudio) (3.21.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->nuscenes-devkit>=1.1.1->nerfstudio) (1.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6->nerfstudio) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (1.4.0)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.3.4->nerfstudio)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading nerfstudio-1.1.5-py3-none-any.whl (580 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.8/580.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gsplat-1.4.0-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nerfacc-0.5.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading splines-0.3.0-py3-none-any.whl (17 kB)\n",
            "Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading viser-0.2.7-py3-none-any.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comet_ml-3.49.5-py3-none-any.whl (725 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.8/725.8 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.3.6-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mediapy-1.2.2-py3-none-any.whl (26 kB)\n",
            "Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nuscenes_devkit-1.1.9-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.6/312.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymeshlab-2023.12.post3-cp311-cp311-manylinux_2_31_x86_64.whl (98.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading python_socketio-5.12.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rawpy-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.3-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.7/931.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.6.4-py3-none-any.whl (708 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.6/708.6 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fpsample-0.3.3-cp311-cp311-manylinux_2_28_x86_64.whl (332 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.0/332.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tensorly-0.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xatlas-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.1-py3-none-any.whl (28 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_engineio-4.11.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading yourdfpy-0.0.57-py3-none-any.whl (22 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading descartes-1.1.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
            "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading manifold3d-3.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl (17.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svg.path-6.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading vhacdx-0.0.8.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: pyliblzfse, fire, pycollada\n",
            "  Building wheel for pyliblzfse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyliblzfse: filename=pyliblzfse-0.4.1-cp311-cp311-linux_x86_64.whl size=88246 sha256=1812a1cc3be0d8ddc8dab153c0eeff935ce9ba776f7abe98dea10e386ff41c98\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/ac/53/c1a09197237c7053cb6884ac1841c275bb05339e246a5038c0\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=1e8ff57d67befe80682837f5da1097e236ab386919055302141e53a27806a4dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "  Building wheel for pycollada (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycollada: filename=pycollada-0.9-py3-none-any.whl size=128464 sha256=46ed2b2f177410adf72354f6e89e38062c05f5a9c87ad55b6c8916edf4d41874\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/f5/25/96914138081f519b3061d82b1306fccf17ea00e00a4b3f8fb2\n",
            "Successfully built pyliblzfse fire pycollada\n",
            "Installing collected packages: pyliblzfse, everett, dash-table, dash-html-components, dash-core-components, appdirs, addict, xxhash, xatlas, wurlitzer, wsproto, widgetsnbextension, werkzeug, wadler-lindig, vhacdx, uri-template, types-python-dateutil, trimesh, svg.path, splines, shtab, semantic-version, rtree, rfc3986-validator, rfc3339-validator, retrying, rawpy, python-json-logger, python-box, pyquaternion, pyngrok, pymeshlab, protobuf, ppft, pox, overrides, opencv-python-headless, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nodeenv, ninja, msgspec, msgpack-numpy, mapbox_earcut, manifold3d, lightning-utilities, json5, jedi, fqdn, fpsample, fire, embreex, dulwich, dill, configobj, configargparse, comm, colorlog, bidict, av, async-lru, tensorly, simple-websocket, pycollada, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-server-terminals, jupyter-client, jaxtyping, flask, arrow, tyro, python-engineio, pathos, nvidia-cusolver-cu12, mediapy, isoduration, ipywidgets, descartes, dash, python-socketio, comet-ml, yourdfpy, torchmetrics, pytorch-msssim, open3d, nerfacc, jupyter-events, gsplat, viser, torch-fidelity, timm, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, nuscenes-devkit, nerfstudio\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.2\n",
            "    Uninstalling python-box-7.3.2:\n",
            "      Successfully uninstalled python-box-7.3.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed addict-2.4.0 appdirs-1.4.4 arrow-1.3.0 async-lru-2.0.5 av-14.2.0 bidict-0.23.1 colorlog-6.9.0 comet-ml-3.49.5 comm-0.2.2 configargparse-1.7 configobj-5.0.9 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 descartes-1.1.0 dill-0.3.9 dulwich-0.22.8 embreex-2.17.7.post6 everett-3.1.0 fire-0.7.0 flask-3.0.3 fpsample-0.3.3 fqdn-1.5.1 gsplat-1.4.0 ipywidgets-8.1.5 isoduration-20.11.0 jaxtyping-0.2.38 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.6 jupyterlab-server-2.27.3 lightning-utilities-0.14.1 manifold3d-3.0.1 mapbox_earcut-1.0.3 mediapy-1.2.2 msgpack-numpy-0.4.8 msgspec-0.19.0 multiprocess-0.70.17 nerfacc-0.5.2 nerfstudio-1.1.5 ninja-1.11.1.3 nodeenv-1.9.1 nuscenes-devkit-1.1.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open3d-0.19.0 opencv-python-headless-4.10.0.84 overrides-7.7.0 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 protobuf-3.20.3 pycollada-0.9 pyliblzfse-0.4.1 pymeshlab-2023.12.post3 pyngrok-7.2.3 pyquaternion-0.9.9 python-box-6.1.0 python-engineio-4.11.2 python-json-logger-3.3.0 python-socketio-5.12.1 pytorch-msssim-1.0.0 rawpy-0.24.0 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rtree-1.4.0 semantic-version-2.10.0 shtab-1.7.1 simple-websocket-1.1.0 splines-0.3.0 svg.path-6.3 tensorly-0.9.0 timm-0.6.7 torch-fidelity-0.3.0 torchmetrics-1.6.3 trimesh-4.6.4 types-python-dateutil-2.9.0.20241206 tyro-0.9.17 uri-template-1.3.0 vhacdx-0.0.8.post2 viser-0.2.7 wadler-lindig-0.1.4 werkzeug-3.0.6 widgetsnbextension-4.0.13 wsproto-1.2.0 wurlitzer-3.1.1 xatlas-0.0.9 xxhash-3.5.0 yourdfpy-0.0.57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "6f080bb419a9485e9fe0e34ad1f692fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MniLf0h8uTg",
        "outputId": "79a0388f-c18f-4cdb-ec46-f6314659d846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "365915 [17/03 20:51:48]\n",
            "365915 [17/03 20:51:48]\n",
            "365915 [17/03 20:51:48]\n",
            "365915 [17/03 20:51:49]\n",
            "Training progress:  29% 2050/7000 [01:00<03:42, 22.29it/s, Loss=0.100279]365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "Training progress:  29% 2060/7000 [01:01<03:42, 22.22it/s, Loss=0.095971]365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:49]\n",
            "Training progress:  30% 2070/7000 [01:01<03:42, 22.16it/s, Loss=0.093298]365915 [17/03 20:51:49]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "Training progress:  30% 2080/7000 [01:02<03:41, 22.18it/s, Loss=0.109824]365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "Training progress:  30% 2090/7000 [01:02<03:41, 22.17it/s, Loss=0.097718]365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:50]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "365915 [17/03 20:51:51]\n",
            "Training progress:  30% 2100/7000 [01:03<03:40, 22.19it/s, Loss=0.099317]402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "Training progress:  30% 2110/7000 [01:03<03:47, 21.47it/s, Loss=0.100171]402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:51]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "Training progress:  30% 2120/7000 [01:04<03:49, 21.25it/s, Loss=0.098118]402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "Training progress:  30% 2130/7000 [01:04<03:50, 21.10it/s, Loss=0.091094]402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:52]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "Training progress:  31% 2140/7000 [01:05<03:51, 21.00it/s, Loss=0.098920]402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "Training progress:  31% 2150/7000 [01:05<03:52, 20.86it/s, Loss=0.100282]402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:53]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "Training progress:  31% 2160/7000 [01:06<03:52, 20.81it/s, Loss=0.093520]402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "Training progress:  31% 2170/7000 [01:06<03:52, 20.76it/s, Loss=0.104924]402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:54]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "Training progress:  31% 2180/7000 [01:07<03:53, 20.66it/s, Loss=0.089311]402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "Training progress:  31% 2190/7000 [01:07<03:53, 20.56it/s, Loss=0.097239]402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:55]\n",
            "402227 [17/03 20:51:56]\n",
            "402227 [17/03 20:51:56]\n",
            "402227 [17/03 20:51:56]\n",
            "Training progress:  31% 2200/7000 [01:08<03:53, 20.59it/s, Loss=0.092139]437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "Training progress:  32% 2210/7000 [01:08<03:59, 20.04it/s, Loss=0.104046]437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:56]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "Training progress:  32% 2220/7000 [01:09<04:00, 19.84it/s, Loss=0.096370]437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "Training progress:  32% 2230/7000 [01:09<04:01, 19.75it/s, Loss=0.090043]437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:57]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "Training progress:  32% 2240/7000 [01:10<04:03, 19.56it/s, Loss=0.094231]437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "Training progress:  32% 2250/7000 [01:10<04:03, 19.47it/s, Loss=0.099671]437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:58]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "Training progress:  32% 2260/7000 [01:11<04:03, 19.50it/s, Loss=0.098430]437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "Training progress:  32% 2270/7000 [01:11<04:02, 19.48it/s, Loss=0.101159]437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:51:59]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "Training progress:  33% 2280/7000 [01:12<04:02, 19.47it/s, Loss=0.097408]437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "Training progress:  33% 2290/7000 [01:12<04:01, 19.47it/s, Loss=0.101833]437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:00]\n",
            "437197 [17/03 20:52:01]\n",
            "437197 [17/03 20:52:01]\n",
            "437197 [17/03 20:52:01]\n",
            "437197 [17/03 20:52:01]\n",
            "437197 [17/03 20:52:01]\n",
            "437197 [17/03 20:52:01]\n",
            "Training progress:  33% 2300/7000 [01:13<04:01, 19.46it/s, Loss=0.090225]471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "Training progress:  33% 2310/7000 [01:13<04:07, 18.91it/s, Loss=0.099356]471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:01]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "Training progress:  33% 2320/7000 [01:14<04:09, 18.76it/s, Loss=0.093775]471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "471663 [17/03 20:52:02]\n",
            "Training progress:  33% 2330/7000 [01:14<04:10, 18.62it/s, Loss=0.103760]471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "Training progress:  33% 2340/7000 [01:15<04:11, 18.55it/s, Loss=0.097966]471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:03]\n",
            "471663 [17/03 20:52:04]\n",
            "Training progress:  34% 2350/7000 [01:15<04:11, 18.47it/s, Loss=0.096856]471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "Training progress:  34% 2360/7000 [01:16<04:11, 18.43it/s, Loss=0.093611]471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:04]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "Training progress:  34% 2370/7000 [01:17<04:11, 18.41it/s, Loss=0.087537]471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "Training progress:  34% 2380/7000 [01:17<04:11, 18.40it/s, Loss=0.087794]471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:05]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "Training progress:  34% 2390/7000 [01:18<04:11, 18.36it/s, Loss=0.090786]471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "471663 [17/03 20:52:06]\n",
            "Training progress:  34% 2400/7000 [01:18<04:10, 18.34it/s, Loss=0.096088]505249 [17/03 20:52:06]\n",
            "505249 [17/03 20:52:06]\n",
            "505249 [17/03 20:52:06]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "Training progress:  34% 2410/7000 [01:19<04:17, 17.83it/s, Loss=0.102170]505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "505249 [17/03 20:52:07]\n",
            "Training progress:  35% 2420/7000 [01:19<04:19, 17.65it/s, Loss=0.085554]505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "Training progress:  35% 2430/7000 [01:20<04:20, 17.55it/s, Loss=0.091480]505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:08]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "Training progress:  35% 2440/7000 [01:20<04:20, 17.50it/s, Loss=0.089090]505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "Training progress:  35% 2450/7000 [01:21<04:20, 17.49it/s, Loss=0.102661]505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:09]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "Training progress:  35% 2460/7000 [01:22<04:20, 17.45it/s, Loss=0.092817]505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "Training progress:  35% 2470/7000 [01:22<04:19, 17.45it/s, Loss=0.095040]505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:10]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "Training progress:  35% 2480/7000 [01:23<04:19, 17.44it/s, Loss=0.094270]505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "505249 [17/03 20:52:11]\n",
            "Training progress:  36% 2490/7000 [01:23<04:18, 17.46it/s, Loss=0.097191]505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "505249 [17/03 20:52:12]\n",
            "Training progress:  36% 2500/7000 [01:24<04:17, 17.47it/s, Loss=0.090853]536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:12]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "Training progress:  36% 2510/7000 [01:25<04:23, 17.02it/s, Loss=0.088091]536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "Training progress:  36% 2520/7000 [01:25<04:25, 16.86it/s, Loss=0.088561]536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:13]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "Training progress:  36% 2530/7000 [01:26<04:26, 16.75it/s, Loss=0.096113]536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "536541 [17/03 20:52:14]\n",
            "Training progress:  36% 2540/7000 [01:26<04:27, 16.67it/s, Loss=0.099613]536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "Training progress:  36% 2550/7000 [01:27<04:27, 16.65it/s, Loss=0.077793]536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:15]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "Training progress:  37% 2560/7000 [01:28<04:27, 16.63it/s, Loss=0.088352]536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "Training progress:  37% 2570/7000 [01:28<04:26, 16.62it/s, Loss=0.089045]536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:16]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "Training progress:  37% 2580/7000 [01:29<04:25, 16.62it/s, Loss=0.100752]536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "536541 [17/03 20:52:17]\n",
            "Training progress:  37% 2590/7000 [01:29<04:25, 16.64it/s, Loss=0.086362]536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "536541 [17/03 20:52:18]\n",
            "Training progress:  37% 2600/7000 [01:30<04:25, 16.59it/s, Loss=0.096541]566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:18]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "Training progress:  37% 2610/7000 [01:31<04:30, 16.20it/s, Loss=0.084549]566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "Training progress:  37% 2620/7000 [01:31<04:32, 16.10it/s, Loss=0.089103]566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:19]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "Training progress:  38% 2630/7000 [01:32<04:32, 16.06it/s, Loss=0.086694]566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:20]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "Training progress:  38% 2640/7000 [01:33<04:31, 16.03it/s, Loss=0.095724]566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "Training progress:  38% 2650/7000 [01:33<04:32, 15.99it/s, Loss=0.089918]566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:21]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "Training progress:  38% 2660/7000 [01:34<04:31, 15.97it/s, Loss=0.084457]566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:22]\n",
            "566227 [17/03 20:52:23]\n",
            "Training progress:  38% 2670/7000 [01:34<04:31, 15.94it/s, Loss=0.090705]566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "Training progress:  38% 2680/7000 [01:35<04:31, 15.94it/s, Loss=0.079271]566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:23]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "Training progress:  38% 2690/7000 [01:36<04:30, 15.92it/s, Loss=0.085355]566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "566227 [17/03 20:52:24]\n",
            "Training progress:  39% 2700/7000 [01:36<04:29, 15.93it/s, Loss=0.091419]596461 [17/03 20:52:24]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "Training progress:  39% 2710/7000 [01:37<04:36, 15.54it/s, Loss=0.092911]596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:25]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "Training progress:  39% 2720/7000 [01:38<04:37, 15.44it/s, Loss=0.090316]596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:26]\n",
            "Training progress:  39% 2730/7000 [01:38<04:37, 15.37it/s, Loss=0.093165]596461 [17/03 20:52:26]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "Training progress:  39% 2740/7000 [01:39<04:38, 15.31it/s, Loss=0.091152]596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:27]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "Training progress:  39% 2750/7000 [01:40<04:37, 15.30it/s, Loss=0.085277]596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "Training progress:  39% 2760/7000 [01:40<04:37, 15.27it/s, Loss=0.089755]596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:28]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "Training progress:  40% 2770/7000 [01:41<04:38, 15.22it/s, Loss=0.099618]596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:29]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "Training progress:  40% 2780/7000 [01:42<04:37, 15.19it/s, Loss=0.096480]596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "Training progress:  40% 2790/7000 [01:42<04:37, 15.18it/s, Loss=0.086236]596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:30]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "596461 [17/03 20:52:31]\n",
            "Training progress:  40% 2800/7000 [01:43<04:36, 15.16it/s, Loss=0.086827]627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:31]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "Training progress:  40% 2810/7000 [01:44<04:43, 14.77it/s, Loss=0.091374]627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:32]\n",
            "Training progress:  40% 2820/7000 [01:44<04:44, 14.67it/s, Loss=0.101117]627475 [17/03 20:52:32]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "Training progress:  40% 2830/7000 [01:45<04:45, 14.61it/s, Loss=0.097823]627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:33]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "Training progress:  41% 2840/7000 [01:46<04:45, 14.58it/s, Loss=0.096550]627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "627475 [17/03 20:52:34]\n",
            "Training progress:  41% 2850/7000 [01:46<04:44, 14.57it/s, Loss=0.088512]627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "Training progress:  41% 2860/7000 [01:47<04:44, 14.56it/s, Loss=0.087009]627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:35]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "Training progress:  41% 2870/7000 [01:48<04:43, 14.59it/s, Loss=0.100442]627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:36]\n",
            "627475 [17/03 20:52:37]\n",
            "Training progress:  41% 2880/7000 [01:48<04:42, 14.58it/s, Loss=0.093585]627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "Training progress:  41% 2890/7000 [01:49<04:42, 14.57it/s, Loss=0.081701]627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:37]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "627475 [17/03 20:52:38]\n",
            "Training progress:  41% 2900/7000 [01:50<04:41, 14.57it/s, Loss=0.090620]656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:38]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "Training progress:  42% 2910/7000 [01:51<04:47, 14.23it/s, Loss=0.082147]656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:39]\n",
            "Training progress:  42% 2920/7000 [01:51<04:49, 14.11it/s, Loss=0.100762]656102 [17/03 20:52:39]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "Training progress:  42% 2930/7000 [01:52<04:49, 14.04it/s, Loss=0.088267]656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:40]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "Training progress:  42% 2940/7000 [01:53<04:50, 13.99it/s, Loss=0.088008]656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:41]\n",
            "656102 [17/03 20:52:42]\n",
            "Training progress:  42% 2950/7000 [01:53<04:50, 13.96it/s, Loss=0.083410]656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "Training progress:  42% 2960/7000 [01:54<04:49, 13.95it/s, Loss=0.089837]656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:42]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "Training progress:  42% 2970/7000 [01:55<04:49, 13.94it/s, Loss=0.084532]656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:43]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "Training progress:  43% 2980/7000 [01:56<04:48, 13.93it/s, Loss=0.092024]656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:44]\n",
            "Training progress:  43% 2990/7000 [01:56<04:48, 13.92it/s, Loss=0.083412]656102 [17/03 20:52:44]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "656102 [17/03 20:52:45]\n",
            "Training progress:  43% 3000/7000 [01:57<04:47, 13.92it/s, Loss=0.081911]682990 [17/03 20:52:45]\n",
            "682990 [17/03 20:52:45]\n",
            "682990 [17/03 20:52:45]\n",
            "682990 [17/03 20:52:45]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "Training progress:  43% 3010/7000 [01:58<04:53, 13.59it/s, Loss=0.095998]682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:46]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "Training progress:  43% 3020/7000 [01:59<04:54, 13.53it/s, Loss=0.093951]682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:47]\n",
            "Training progress:  43% 3030/7000 [01:59<04:53, 13.53it/s, Loss=0.092017]682990 [17/03 20:52:47]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "Training progress:  43% 3040/7000 [02:00<04:53, 13.49it/s, Loss=0.086676]682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:48]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "Training progress:  44% 3050/7000 [02:01<04:53, 13.46it/s, Loss=0.091031]682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:49]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "Training progress:  44% 3060/7000 [02:02<04:52, 13.45it/s, Loss=0.087769]682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:50]\n",
            "Training progress:  44% 3070/7000 [02:02<04:52, 13.43it/s, Loss=0.074357]682990 [17/03 20:52:50]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "Training progress:  44% 3080/7000 [02:03<04:52, 13.42it/s, Loss=0.083471]682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:51]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "Training progress:  44% 3090/7000 [02:04<04:51, 13.41it/s, Loss=0.080928]682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:52]\n",
            "682990 [17/03 20:52:53]\n",
            "682990 [17/03 20:52:53]\n",
            "Training progress:  44% 3100/7000 [02:05<04:51, 13.39it/s, Loss=0.086759]700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:53]\n",
            "Training progress:  44% 3110/7000 [02:05<04:53, 13.27it/s, Loss=0.109436]700739 [17/03 20:52:53]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "Training progress:  45% 3120/7000 [02:06<04:51, 13.31it/s, Loss=0.108267]700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:54]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "Training progress:  45% 3130/7000 [02:07<04:50, 13.31it/s, Loss=0.097457]700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:55]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "Training progress:  45% 3140/7000 [02:08<04:49, 13.33it/s, Loss=0.107074]700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:56]\n",
            "Training progress:  45% 3150/7000 [02:08<04:48, 13.35it/s, Loss=0.105871]700739 [17/03 20:52:56]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "Training progress:  45% 3160/7000 [02:09<04:47, 13.35it/s, Loss=0.086740]700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:57]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "Training progress:  45% 3170/7000 [02:10<04:46, 13.36it/s, Loss=0.096253]700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:58]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "Training progress:  45% 3180/7000 [02:11<04:45, 13.36it/s, Loss=0.084132]700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:52:59]\n",
            "Training progress:  46% 3190/7000 [02:11<04:45, 13.35it/s, Loss=0.098547]700739 [17/03 20:52:59]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "700739 [17/03 20:53:00]\n",
            "Training progress:  46% 3200/7000 [02:12<04:44, 13.37it/s, Loss=0.102634]736224 [17/03 20:53:00]\n",
            "736224 [17/03 20:53:00]\n",
            "736224 [17/03 20:53:00]\n",
            "736224 [17/03 20:53:00]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "Training progress:  46% 3210/7000 [02:13<04:49, 13.08it/s, Loss=0.101254]736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:01]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "Training progress:  46% 3220/7000 [02:14<04:50, 13.03it/s, Loss=0.122466]736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "736224 [17/03 20:53:02]\n",
            "Training progress:  46% 3230/7000 [02:14<04:49, 13.01it/s, Loss=0.099136]736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "Training progress:  46% 3240/7000 [02:15<04:49, 13.00it/s, Loss=0.104482]736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:03]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "Training progress:  46% 3250/7000 [02:16<04:49, 12.96it/s, Loss=0.100525]736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:04]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "Training progress:  47% 3260/7000 [02:17<04:49, 12.94it/s, Loss=0.094505]736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:05]\n",
            "736224 [17/03 20:53:06]\n",
            "Training progress:  47% 3270/7000 [02:17<04:49, 12.90it/s, Loss=0.101074]736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:06]\n",
            "Training progress:  47% 3280/7000 [02:18<04:48, 12.88it/s, Loss=0.096328]736224 [17/03 20:53:06]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "Training progress:  47% 3290/7000 [02:19<04:48, 12.85it/s, Loss=0.096348]736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:07]\n",
            "736224 [17/03 20:53:08]\n",
            "736224 [17/03 20:53:08]\n",
            "736224 [17/03 20:53:08]\n",
            "736224 [17/03 20:53:08]\n",
            "736224 [17/03 20:53:08]\n",
            "736224 [17/03 20:53:08]\n",
            "Training progress:  47% 3300/7000 [02:20<04:48, 12.84it/s, Loss=0.089799]767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:08]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "Training progress:  47% 3310/7000 [02:21<04:54, 12.55it/s, Loss=0.099323]767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:09]\n",
            "767275 [17/03 20:53:10]\n",
            "Training progress:  47% 3320/7000 [02:21<04:54, 12.48it/s, Loss=0.098894]767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:10]\n",
            "Training progress:  48% 3330/7000 [02:22<04:54, 12.45it/s, Loss=0.085569]767275 [17/03 20:53:10]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "Training progress:  48% 3340/7000 [02:23<04:55, 12.40it/s, Loss=0.088006]767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:11]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "Training progress:  48% 3350/7000 [02:24<04:54, 12.39it/s, Loss=0.084131]767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:12]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "Training progress:  48% 3360/7000 [02:25<04:53, 12.39it/s, Loss=0.081792]767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:13]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "Training progress:  48% 3370/7000 [02:26<04:53, 12.38it/s, Loss=0.085246]767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:14]\n",
            "Training progress:  48% 3380/7000 [02:26<04:52, 12.38it/s, Loss=0.081771]767275 [17/03 20:53:14]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "Training progress:  48% 3390/7000 [02:27<04:51, 12.38it/s, Loss=0.092354]767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:15]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "767275 [17/03 20:53:16]\n",
            "Training progress:  49% 3400/7000 [02:28<04:50, 12.39it/s, Loss=0.087633]799210 [17/03 20:53:16]\n",
            "799210 [17/03 20:53:16]\n",
            "799210 [17/03 20:53:16]\n",
            "799210 [17/03 20:53:16]\n",
            "799210 [17/03 20:53:16]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "Training progress:  49% 3410/7000 [02:29<04:55, 12.13it/s, Loss=0.106818]799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:17]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "Training progress:  49% 3420/7000 [02:30<04:56, 12.05it/s, Loss=0.099304]799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:18]\n",
            "799210 [17/03 20:53:19]\n",
            "Training progress:  49% 3430/7000 [02:30<04:57, 11.99it/s, Loss=0.090959]799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "799210 [17/03 20:53:19]\n",
            "Training progress:  49% 3440/7000 [02:31<04:57, 11.96it/s, Loss=0.093606]799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "Training progress:  49% 3450/7000 [02:32<04:57, 11.92it/s, Loss=0.092026]799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:20]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "Training progress:  49% 3460/7000 [02:33<04:57, 11.89it/s, Loss=0.091533]799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:21]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "Training progress:  50% 3470/7000 [02:34<04:56, 11.91it/s, Loss=0.082584]799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:22]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "Training progress:  50% 3480/7000 [02:35<04:56, 11.89it/s, Loss=0.082844]799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:23]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "Training progress:  50% 3490/7000 [02:36<04:55, 11.89it/s, Loss=0.081432]799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "799210 [17/03 20:53:24]\n",
            "Training progress:  50% 3500/7000 [02:36<04:54, 11.89it/s, Loss=0.094421]831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:25]\n",
            "Training progress:  50% 3510/7000 [02:37<04:59, 11.65it/s, Loss=0.095546]831571 [17/03 20:53:25]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "Training progress:  50% 3520/7000 [02:38<05:00, 11.59it/s, Loss=0.089794]831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:26]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "Training progress:  50% 3530/7000 [02:39<05:00, 11.56it/s, Loss=0.088081]831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:27]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "Training progress:  51% 3540/7000 [02:40<05:00, 11.53it/s, Loss=0.078454]831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:28]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "Training progress:  51% 3550/7000 [02:41<04:59, 11.51it/s, Loss=0.088079]831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:29]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "Training progress:  51% 3560/7000 [02:42<04:58, 11.51it/s, Loss=0.091240]831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:30]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "Training progress:  51% 3570/7000 [02:43<04:58, 11.49it/s, Loss=0.082790]831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "831571 [17/03 20:53:31]\n",
            "Training progress:  51% 3580/7000 [02:43<04:58, 11.46it/s, Loss=0.081959]831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:32]\n",
            "Training progress:  51% 3590/7000 [02:44<04:58, 11.44it/s, Loss=0.094035]831571 [17/03 20:53:32]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "831571 [17/03 20:53:33]\n",
            "Training progress:  51% 3600/7000 [02:45<04:57, 11.44it/s, Loss=0.079324]864321 [17/03 20:53:33]\n",
            "864321 [17/03 20:53:33]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "Training progress:  52% 3610/7000 [02:46<05:01, 11.23it/s, Loss=0.087370]864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:34]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "Training progress:  52% 3620/7000 [02:47<05:02, 11.19it/s, Loss=0.079499]864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:35]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "Training progress:  52% 3630/7000 [02:48<05:02, 11.16it/s, Loss=0.085164]864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:36]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "Training progress:  52% 3640/7000 [02:49<05:01, 11.13it/s, Loss=0.095871]864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:37]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "Training progress:  52% 3650/7000 [02:50<05:01, 11.12it/s, Loss=0.084829]864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:38]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "Training progress:  52% 3660/7000 [02:51<05:00, 11.12it/s, Loss=0.082965]864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:39]\n",
            "864321 [17/03 20:53:40]\n",
            "Training progress:  52% 3670/7000 [02:51<04:59, 11.10it/s, Loss=0.084094]864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "864321 [17/03 20:53:40]\n",
            "Training progress:  53% 3680/7000 [02:52<04:59, 11.10it/s, Loss=0.079275]864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:41]\n",
            "Training progress:  53% 3690/7000 [02:53<04:58, 11.10it/s, Loss=0.080551]864321 [17/03 20:53:41]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "864321 [17/03 20:53:42]\n",
            "Training progress:  53% 3700/7000 [02:54<04:57, 11.10it/s, Loss=0.079663]894728 [17/03 20:53:42]\n",
            "894728 [17/03 20:53:42]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "Training progress:  53% 3710/7000 [02:55<05:02, 10.89it/s, Loss=0.084257]894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:43]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "Training progress:  53% 3720/7000 [02:56<05:02, 10.85it/s, Loss=0.087554]894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:44]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "Training progress:  53% 3730/7000 [02:57<05:02, 10.82it/s, Loss=0.087301]894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:45]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "Training progress:  53% 3740/7000 [02:58<05:01, 10.80it/s, Loss=0.081118]894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:46]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "Training progress:  54% 3750/7000 [02:59<05:00, 10.80it/s, Loss=0.082223]894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:47]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "Training progress:  54% 3760/7000 [03:00<04:59, 10.80it/s, Loss=0.075981]894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:48]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "Training progress:  54% 3770/7000 [03:01<04:59, 10.80it/s, Loss=0.089532]894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:49]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "Training progress:  54% 3780/7000 [03:02<04:58, 10.80it/s, Loss=0.071729]894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:50]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "Training progress:  54% 3790/7000 [03:03<04:57, 10.79it/s, Loss=0.086025]894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:51]\n",
            "894728 [17/03 20:53:52]\n",
            "Training progress:  54% 3800/7000 [03:04<04:57, 10.77it/s, Loss=0.081262]922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:52]\n",
            "922862 [17/03 20:53:53]\n",
            "Training progress:  54% 3810/7000 [03:04<05:02, 10.56it/s, Loss=0.083367]922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:53]\n",
            "922862 [17/03 20:53:54]\n",
            "Training progress:  55% 3820/7000 [03:05<05:02, 10.53it/s, Loss=0.074018]922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "922862 [17/03 20:53:54]\n",
            "Training progress:  55% 3830/7000 [03:06<05:01, 10.52it/s, Loss=0.086915]922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "922862 [17/03 20:53:55]\n",
            "Training progress:  55% 3840/7000 [03:07<05:00, 10.50it/s, Loss=0.081160]922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:56]\n",
            "Training progress:  55% 3850/7000 [03:08<05:00, 10.49it/s, Loss=0.075060]922862 [17/03 20:53:56]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:57]\n",
            "Training progress:  55% 3860/7000 [03:09<04:59, 10.47it/s, Loss=0.073410]922862 [17/03 20:53:57]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "Training progress:  55% 3870/7000 [03:10<04:58, 10.47it/s, Loss=0.075202]922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:58]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "Training progress:  55% 3880/7000 [03:11<04:58, 10.46it/s, Loss=0.085933]922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:53:59]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "Training progress:  56% 3890/7000 [03:12<04:57, 10.46it/s, Loss=0.076284]922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:00]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "922862 [17/03 20:54:01]\n",
            "Training progress:  56% 3900/7000 [03:13<04:56, 10.46it/s, Loss=0.081704]951282 [17/03 20:54:01]\n",
            "951282 [17/03 20:54:01]\n",
            "951282 [17/03 20:54:01]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "Training progress:  56% 3910/7000 [03:14<05:01, 10.25it/s, Loss=0.090593]951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:02]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "Training progress:  56% 3920/7000 [03:15<05:01, 10.21it/s, Loss=0.080518]951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:03]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "Training progress:  56% 3930/7000 [03:16<05:01, 10.20it/s, Loss=0.098114]951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:04]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "Training progress:  56% 3940/7000 [03:17<05:00, 10.18it/s, Loss=0.085092]951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:05]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "Training progress:  56% 3950/7000 [03:18<05:00, 10.16it/s, Loss=0.084833]951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:06]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "Training progress:  57% 3960/7000 [03:19<04:59, 10.15it/s, Loss=0.071172]951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:07]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "Training progress:  57% 3970/7000 [03:20<04:58, 10.15it/s, Loss=0.087173]951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:08]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "Training progress:  57% 3980/7000 [03:21<04:57, 10.13it/s, Loss=0.083400]951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:09]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "Training progress:  57% 3990/7000 [03:22<04:56, 10.14it/s, Loss=0.081012]951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:10]\n",
            "951282 [17/03 20:54:11]\n",
            "951282 [17/03 20:54:11]\n",
            "951282 [17/03 20:54:11]\n",
            "951282 [17/03 20:54:11]\n",
            "951282 [17/03 20:54:11]\n",
            "951282 [17/03 20:54:11]\n",
            "Training progress:  57% 4000/7000 [03:23<04:55, 10.14it/s, Loss=0.085512]979628 [17/03 20:54:11]\n",
            "979628 [17/03 20:54:11]\n",
            "979628 [17/03 20:54:11]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "Training progress:  57% 4010/7000 [03:24<05:00,  9.96it/s, Loss=0.090336]979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:12]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "Training progress:  57% 4020/7000 [03:25<04:59,  9.94it/s, Loss=0.078172]979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:13]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "Training progress:  58% 4030/7000 [03:26<04:59,  9.93it/s, Loss=0.073227]979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:14]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "Training progress:  58% 4040/7000 [03:27<04:58,  9.93it/s, Loss=0.078261]979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:15]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "Training progress:  58% 4050/7000 [03:28<04:57,  9.93it/s, Loss=0.078450]979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:16]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "Training progress:  58% 4060/7000 [03:29<04:55,  9.93it/s, Loss=0.086801]979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:17]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "Training progress:  58% 4070/7000 [03:30<04:55,  9.92it/s, Loss=0.084615]979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:18]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "Training progress:  58% 4080/7000 [03:31<04:54,  9.90it/s, Loss=0.074619]979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:19]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "Training progress:  58% 4090/7000 [03:32<04:53,  9.90it/s, Loss=0.078241]979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:20]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "979628 [17/03 20:54:21]\n",
            "Training progress:  59% 4100/7000 [03:33<04:52,  9.91it/s, Loss=0.083716]1004443 [17/03 20:54:21]\n",
            "1004443 [17/03 20:54:21]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "Training progress:  59% 4110/7000 [03:34<04:56,  9.75it/s, Loss=0.071526]1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:22]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "Training progress:  59% 4120/7000 [03:35<04:55,  9.74it/s, Loss=0.095644]1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:23]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:24]\n",
            "Training progress:  59% 4130/7000 [03:36<04:55,  9.71it/s, Loss=0.084585]1004443 [17/03 20:54:24]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:25]\n",
            "Training progress:  59% 4140/7000 [03:37<04:54,  9.70it/s, Loss=0.072463]1004443 [17/03 20:54:25]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:26]\n",
            "Training progress:  59% 4150/7000 [03:38<04:53,  9.70it/s, Loss=0.076498]1004443 [17/03 20:54:26]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "1004443 [17/03 20:54:27]\n",
            "Training progress:  59% 4160/7000 [03:39<04:53,  9.67it/s, Loss=0.085430]1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "1004443 [17/03 20:54:28]\n",
            "Training progress:  60% 4170/7000 [03:40<04:52,  9.68it/s, Loss=0.082913]1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "1004443 [17/03 20:54:29]\n",
            "Training progress:  60% 4180/7000 [03:41<04:51,  9.68it/s, Loss=0.069757]1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:30]\n",
            "1004443 [17/03 20:54:31]\n",
            "Training progress:  60% 4190/7000 [03:42<04:50,  9.68it/s, Loss=0.078642]1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:31]\n",
            "1004443 [17/03 20:54:32]\n",
            "Training progress:  60% 4200/7000 [03:43<04:49,  9.67it/s, Loss=0.072344]1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:32]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "Training progress:  60% 4210/7000 [03:45<04:53,  9.51it/s, Loss=0.091295]1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:33]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "Training progress:  60% 4220/7000 [03:46<04:52,  9.50it/s, Loss=0.095700]1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:34]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "Training progress:  60% 4230/7000 [03:47<04:51,  9.49it/s, Loss=0.077899]1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:35]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "Training progress:  61% 4240/7000 [03:48<04:50,  9.49it/s, Loss=0.075041]1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:36]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "Training progress:  61% 4250/7000 [03:49<04:49,  9.49it/s, Loss=0.079366]1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:37]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "Training progress:  61% 4260/7000 [03:50<04:49,  9.47it/s, Loss=0.072394]1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:38]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "Training progress:  61% 4270/7000 [03:51<04:48,  9.46it/s, Loss=0.080672]1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:39]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "Training progress:  61% 4280/7000 [03:52<04:47,  9.46it/s, Loss=0.072529]1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:40]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "Training progress:  61% 4290/7000 [03:53<04:46,  9.46it/s, Loss=0.085785]1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:41]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "1028498 [17/03 20:54:42]\n",
            "Training progress:  61% 4300/7000 [03:54<04:45,  9.45it/s, Loss=0.076400]1055476 [17/03 20:54:42]\n",
            "1055476 [17/03 20:54:42]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:43]\n",
            "Training progress:  62% 4310/7000 [03:55<04:49,  9.30it/s, Loss=0.083337]1055476 [17/03 20:54:43]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:44]\n",
            "Training progress:  62% 4320/7000 [03:56<04:48,  9.29it/s, Loss=0.082217]1055476 [17/03 20:54:44]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "1055476 [17/03 20:54:45]\n",
            "Training progress:  62% 4330/7000 [03:57<04:48,  9.27it/s, Loss=0.095253]1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:46]\n",
            "1055476 [17/03 20:54:47]\n",
            "Training progress:  62% 4340/7000 [03:58<04:46,  9.27it/s, Loss=0.076537]1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:47]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "Training progress:  62% 4350/7000 [04:00<04:46,  9.26it/s, Loss=0.083366]1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:48]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "Training progress:  62% 4360/7000 [04:01<04:44,  9.27it/s, Loss=0.076482]1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:49]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "Training progress:  62% 4370/7000 [04:02<04:43,  9.27it/s, Loss=0.070414]1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:50]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "Training progress:  63% 4380/7000 [04:03<04:43,  9.26it/s, Loss=0.075542]1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:51]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "Training progress:  63% 4390/7000 [04:04<04:41,  9.26it/s, Loss=0.067828]1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:52]\n",
            "1055476 [17/03 20:54:53]\n",
            "1055476 [17/03 20:54:53]\n",
            "1055476 [17/03 20:54:53]\n",
            "1055476 [17/03 20:54:53]\n",
            "1055476 [17/03 20:54:53]\n",
            "Training progress:  63% 4400/7000 [04:05<04:41,  9.25it/s, Loss=0.076797]1077762 [17/03 20:54:53]\n",
            "1077762 [17/03 20:54:53]\n",
            "1077762 [17/03 20:54:53]\n",
            "1077762 [17/03 20:54:53]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "Training progress:  63% 4410/7000 [04:06<04:44,  9.10it/s, Loss=0.112291]1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:54]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "Training progress:  63% 4420/7000 [04:07<04:43,  9.10it/s, Loss=0.074530]1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:55]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:56]\n",
            "Training progress:  63% 4430/7000 [04:08<04:42,  9.08it/s, Loss=0.076321]1077762 [17/03 20:54:56]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "1077762 [17/03 20:54:57]\n",
            "Training progress:  63% 4440/7000 [04:09<04:41,  9.08it/s, Loss=0.074289]1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:58]\n",
            "1077762 [17/03 20:54:59]\n",
            "Training progress:  64% 4450/7000 [04:10<04:40,  9.09it/s, Loss=0.082863]1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:54:59]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "Training progress:  64% 4460/7000 [04:12<04:39,  9.09it/s, Loss=0.078686]1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:00]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "Training progress:  64% 4470/7000 [04:13<04:38,  9.08it/s, Loss=0.073444]1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:01]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "Training progress:  64% 4480/7000 [04:14<04:37,  9.08it/s, Loss=0.080950]1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:02]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "Training progress:  64% 4490/7000 [04:15<04:36,  9.07it/s, Loss=0.078592]1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:03]\n",
            "1077762 [17/03 20:55:04]\n",
            "1077762 [17/03 20:55:04]\n",
            "1077762 [17/03 20:55:04]\n",
            "1077762 [17/03 20:55:04]\n",
            "1077762 [17/03 20:55:04]\n",
            "1077762 [17/03 20:55:04]\n",
            "Training progress:  64% 4500/7000 [04:16<04:35,  9.07it/s, Loss=0.077060]1097970 [17/03 20:55:04]\n",
            "1097970 [17/03 20:55:04]\n",
            "1097970 [17/03 20:55:04]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "Training progress:  64% 4510/7000 [04:17<04:38,  8.94it/s, Loss=0.078579]1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:05]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:06]\n",
            "Training progress:  65% 4520/7000 [04:18<04:37,  8.94it/s, Loss=0.074027]1097970 [17/03 20:55:06]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "1097970 [17/03 20:55:07]\n",
            "Training progress:  65% 4530/7000 [04:19<04:36,  8.93it/s, Loss=0.080245]1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:08]\n",
            "1097970 [17/03 20:55:09]\n",
            "Training progress:  65% 4540/7000 [04:21<04:35,  8.92it/s, Loss=0.077847]1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:09]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "Training progress:  65% 4550/7000 [04:22<04:34,  8.93it/s, Loss=0.072235]1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:10]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "Training progress:  65% 4560/7000 [04:23<04:33,  8.91it/s, Loss=0.074971]1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:11]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "Training progress:  65% 4570/7000 [04:24<04:32,  8.92it/s, Loss=0.072340]1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:12]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "Training progress:  65% 4580/7000 [04:25<04:31,  8.92it/s, Loss=0.072259]1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:13]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "Training progress:  66% 4590/7000 [04:26<04:30,  8.92it/s, Loss=0.082883]1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:14]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "1097970 [17/03 20:55:15]\n",
            "Training progress:  66% 4600/7000 [04:27<04:29,  8.92it/s, Loss=0.065568]1121458 [17/03 20:55:15]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "1121458 [17/03 20:55:16]\n",
            "Training progress:  66% 4610/7000 [04:28<04:32,  8.77it/s, Loss=0.071279]1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:17]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "Training progress:  66% 4620/7000 [04:30<04:31,  8.76it/s, Loss=0.068505]1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:18]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "Training progress:  66% 4630/7000 [04:31<04:30,  8.75it/s, Loss=0.080640]1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:19]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "Training progress:  66% 4640/7000 [04:32<04:29,  8.75it/s, Loss=0.070308]1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:20]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "Training progress:  66% 4650/7000 [04:33<04:28,  8.74it/s, Loss=0.069612]1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:21]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "Training progress:  67% 4660/7000 [04:34<04:28,  8.73it/s, Loss=0.071492]1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:22]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:23]\n",
            "Training progress:  67% 4670/7000 [04:35<04:27,  8.72it/s, Loss=0.069909]1121458 [17/03 20:55:23]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:24]\n",
            "1121458 [17/03 20:55:25]\n",
            "Training progress:  67% 4680/7000 [04:36<04:26,  8.71it/s, Loss=0.081513]1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:25]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "Training progress:  67% 4690/7000 [04:38<04:25,  8.71it/s, Loss=0.070415]1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:26]\n",
            "1121458 [17/03 20:55:27]\n",
            "1121458 [17/03 20:55:27]\n",
            "1121458 [17/03 20:55:27]\n",
            "Training progress:  67% 4700/7000 [04:39<04:24,  8.70it/s, Loss=0.071925]1140101 [17/03 20:55:27]\n",
            "1140101 [17/03 20:55:27]\n",
            "1140101 [17/03 20:55:27]\n",
            "1140101 [17/03 20:55:27]\n",
            "1140101 [17/03 20:55:27]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "Training progress:  67% 4710/7000 [04:40<04:26,  8.59it/s, Loss=0.077449]1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:28]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "Training progress:  67% 4720/7000 [04:41<04:25,  8.59it/s, Loss=0.078897]1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:29]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:30]\n",
            "Training progress:  68% 4730/7000 [04:42<04:24,  8.59it/s, Loss=0.074938]1140101 [17/03 20:55:30]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:31]\n",
            "1140101 [17/03 20:55:32]\n",
            "Training progress:  68% 4740/7000 [04:43<04:23,  8.59it/s, Loss=0.070082]1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:32]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "Training progress:  68% 4750/7000 [04:45<04:21,  8.59it/s, Loss=0.070532]1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:33]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "Training progress:  68% 4760/7000 [04:46<04:20,  8.59it/s, Loss=0.083595]1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:34]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "Training progress:  68% 4770/7000 [04:47<04:19,  8.58it/s, Loss=0.084539]1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:35]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "Training progress:  68% 4780/7000 [04:48<04:18,  8.59it/s, Loss=0.072585]1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:36]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:37]\n",
            "Training progress:  68% 4790/7000 [04:49<04:17,  8.59it/s, Loss=0.075294]1140101 [17/03 20:55:37]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:38]\n",
            "1140101 [17/03 20:55:39]\n",
            "Training progress:  69% 4800/7000 [04:50<04:16,  8.59it/s, Loss=0.068964]1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:39]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "Training progress:  69% 4810/7000 [04:52<04:18,  8.46it/s, Loss=0.077604]1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:40]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "Training progress:  69% 4820/7000 [04:53<04:17,  8.45it/s, Loss=0.072365]1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:41]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "Training progress:  69% 4830/7000 [04:54<04:16,  8.45it/s, Loss=0.078128]1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:42]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:43]\n",
            "Training progress:  69% 4840/7000 [04:55<04:15,  8.45it/s, Loss=0.070701]1159513 [17/03 20:55:43]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "1159513 [17/03 20:55:44]\n",
            "Training progress:  69% 4850/7000 [04:56<04:14,  8.44it/s, Loss=0.070012]1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:45]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "Training progress:  69% 4860/7000 [04:58<04:13,  8.43it/s, Loss=0.073217]1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:46]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "Training progress:  70% 4870/7000 [04:59<04:12,  8.44it/s, Loss=0.071237]1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:47]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "Training progress:  70% 4880/7000 [05:00<04:10,  8.45it/s, Loss=0.086206]1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:48]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "Training progress:  70% 4890/7000 [05:01<04:09,  8.46it/s, Loss=0.067174]1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:49]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "1159513 [17/03 20:55:50]\n",
            "Training progress:  70% 4900/7000 [05:02<04:08,  8.47it/s, Loss=0.068789]1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:51]\n",
            "1177367 [17/03 20:55:52]\n",
            "Training progress:  70% 4910/7000 [05:04<04:10,  8.35it/s, Loss=0.078225]1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:52]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "Training progress:  70% 4920/7000 [05:05<04:09,  8.34it/s, Loss=0.072400]1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:53]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "Training progress:  70% 4930/7000 [05:06<04:08,  8.33it/s, Loss=0.071872]1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:54]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "Training progress:  71% 4940/7000 [05:07<04:07,  8.33it/s, Loss=0.074640]1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:55]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "1177367 [17/03 20:55:56]\n",
            "Training progress:  71% 4950/7000 [05:08<04:06,  8.33it/s, Loss=0.107954]1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:57]\n",
            "1177367 [17/03 20:55:58]\n",
            "Training progress:  71% 4960/7000 [05:10<04:04,  8.33it/s, Loss=0.071826]1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:58]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "Training progress:  71% 4970/7000 [05:11<04:03,  8.34it/s, Loss=0.065273]1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:55:59]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "Training progress:  71% 4980/7000 [05:12<04:02,  8.33it/s, Loss=0.076825]1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:00]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "Training progress:  71% 4990/7000 [05:13<04:01,  8.33it/s, Loss=0.067973]1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:01]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "1177367 [17/03 20:56:02]\n",
            "Training progress:  71% 5000/7000 [05:14<04:00,  8.33it/s, Loss=0.073057]1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:03]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "Training progress:  72% 5010/7000 [05:16<04:02,  8.21it/s, Loss=0.089111]1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:04]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "Training progress:  72% 5020/7000 [05:17<04:01,  8.21it/s, Loss=0.074807]1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:05]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "Training progress:  72% 5030/7000 [05:18<04:00,  8.20it/s, Loss=0.075357]1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:06]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:07]\n",
            "Training progress:  72% 5040/7000 [05:19<03:59,  8.20it/s, Loss=0.065203]1197927 [17/03 20:56:07]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:08]\n",
            "1197927 [17/03 20:56:09]\n",
            "Training progress:  72% 5050/7000 [05:21<03:58,  8.19it/s, Loss=0.071079]1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:09]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "Training progress:  72% 5060/7000 [05:22<03:56,  8.19it/s, Loss=0.072624]1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:10]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "Training progress:  72% 5070/7000 [05:23<03:55,  8.20it/s, Loss=0.062647]1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:11]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "Training progress:  73% 5080/7000 [05:24<03:53,  8.21it/s, Loss=0.069694]1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:12]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "1197927 [17/03 20:56:13]\n",
            "Training progress:  73% 5090/7000 [05:25<03:52,  8.22it/s, Loss=0.065851]1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:14]\n",
            "1197927 [17/03 20:56:15]\n",
            "1197927 [17/03 20:56:15]\n",
            "Training progress:  73% 5100/7000 [05:27<03:51,  8.21it/s, Loss=0.072830]1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:15]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "Training progress:  73% 5110/7000 [05:28<03:53,  8.10it/s, Loss=0.078071]1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:16]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "Training progress:  73% 5120/7000 [05:29<03:51,  8.11it/s, Loss=0.069407]1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:17]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "1215166 [17/03 20:56:18]\n",
            "Training progress:  73% 5130/7000 [05:30<03:50,  8.10it/s, Loss=0.072658]1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:19]\n",
            "1215166 [17/03 20:56:20]\n",
            "Training progress:  73% 5140/7000 [05:32<03:49,  8.11it/s, Loss=0.068658]1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:20]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "Training progress:  74% 5150/7000 [05:33<03:48,  8.10it/s, Loss=0.065359]1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:21]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "Training progress:  74% 5160/7000 [05:34<03:47,  8.09it/s, Loss=0.078740]1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:22]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:23]\n",
            "Training progress:  74% 5170/7000 [05:35<03:46,  8.08it/s, Loss=0.076665]1215166 [17/03 20:56:23]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:24]\n",
            "1215166 [17/03 20:56:25]\n",
            "Training progress:  74% 5180/7000 [05:37<03:45,  8.09it/s, Loss=0.066080]1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:25]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "Training progress:  74% 5190/7000 [05:38<03:43,  8.10it/s, Loss=0.072718]1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:26]\n",
            "1215166 [17/03 20:56:27]\n",
            "1215166 [17/03 20:56:27]\n",
            "1215166 [17/03 20:56:27]\n",
            "1215166 [17/03 20:56:27]\n",
            "1215166 [17/03 20:56:27]\n",
            "Training progress:  74% 5200/7000 [05:39<03:42,  8.10it/s, Loss=0.070122]1229243 [17/03 20:56:27]\n",
            "1229243 [17/03 20:56:27]\n",
            "1229243 [17/03 20:56:27]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:28]\n",
            "Training progress:  74% 5210/7000 [05:40<03:43,  7.99it/s, Loss=0.077419]1229243 [17/03 20:56:28]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:29]\n",
            "1229243 [17/03 20:56:30]\n",
            "Training progress:  75% 5220/7000 [05:42<03:42,  7.99it/s, Loss=0.074815]1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:30]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "Training progress:  75% 5230/7000 [05:43<03:41,  8.00it/s, Loss=0.068888]1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:31]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "Training progress:  75% 5240/7000 [05:44<03:39,  8.00it/s, Loss=0.067469]1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:32]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:33]\n",
            "Training progress:  75% 5250/7000 [05:45<03:38,  7.99it/s, Loss=0.076416]1229243 [17/03 20:56:33]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:34]\n",
            "1229243 [17/03 20:56:35]\n",
            "Training progress:  75% 5260/7000 [05:47<03:37,  7.99it/s, Loss=0.067659]1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:35]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "Training progress:  75% 5270/7000 [05:48<03:36,  8.00it/s, Loss=0.069916]1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:36]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "Training progress:  75% 5280/7000 [05:49<03:35,  8.00it/s, Loss=0.061405]1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:37]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:38]\n",
            "Training progress:  76% 5290/7000 [05:50<03:33,  7.99it/s, Loss=0.066582]1229243 [17/03 20:56:38]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:39]\n",
            "1229243 [17/03 20:56:40]\n",
            "Training progress:  76% 5300/7000 [05:52<03:32,  7.99it/s, Loss=0.071487]1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:40]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "Training progress:  76% 5310/7000 [05:53<03:34,  7.88it/s, Loss=0.066299]1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:41]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "Training progress:  76% 5320/7000 [05:54<03:33,  7.87it/s, Loss=0.070777]1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:42]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "1249935 [17/03 20:56:43]\n",
            "Training progress:  76% 5330/7000 [05:55<03:32,  7.85it/s, Loss=0.079063]1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:44]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "Training progress:  76% 5340/7000 [05:57<03:31,  7.85it/s, Loss=0.068284]1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:45]\n",
            "1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:46]\n",
            "Training progress:  76% 5350/7000 [05:58<03:30,  7.84it/s, Loss=0.066910]1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:46]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:47]\n",
            "Training progress:  77% 5360/7000 [05:59<03:28,  7.85it/s, Loss=0.074012]1249935 [17/03 20:56:47]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:48]\n",
            "1249935 [17/03 20:56:49]\n",
            "Training progress:  77% 5370/7000 [06:00<03:27,  7.86it/s, Loss=0.078017]1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:49]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "Training progress:  77% 5380/7000 [06:02<03:25,  7.87it/s, Loss=0.068075]1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:50]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "Training progress:  77% 5390/7000 [06:03<03:24,  7.87it/s, Loss=0.070904]1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:51]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "1249935 [17/03 20:56:52]\n",
            "Training progress:  77% 5400/7000 [06:04<03:23,  7.87it/s, Loss=0.068085]1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:53]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "Training progress:  77% 5410/7000 [06:06<03:24,  7.78it/s, Loss=0.067136]1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:54]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "Training progress:  77% 5420/7000 [06:07<03:22,  7.79it/s, Loss=0.080169]1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:55]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "Training progress:  78% 5430/7000 [06:08<03:21,  7.79it/s, Loss=0.070901]1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:56]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:57]\n",
            "1261390 [17/03 20:56:58]\n",
            "Training progress:  78% 5440/7000 [06:09<03:20,  7.79it/s, Loss=0.069275]1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:58]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "Training progress:  78% 5450/7000 [06:11<03:19,  7.79it/s, Loss=0.068136]1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:56:59]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "Training progress:  78% 5460/7000 [06:12<03:17,  7.79it/s, Loss=0.063537]1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:00]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:01]\n",
            "Training progress:  78% 5470/7000 [06:13<03:16,  7.79it/s, Loss=0.070229]1261390 [17/03 20:57:01]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:02]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "Training progress:  78% 5480/7000 [06:15<03:15,  7.79it/s, Loss=0.072755]1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:03]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "Training progress:  78% 5490/7000 [06:16<03:13,  7.80it/s, Loss=0.066024]1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:04]\n",
            "1261390 [17/03 20:57:05]\n",
            "1261390 [17/03 20:57:05]\n",
            "1261390 [17/03 20:57:05]\n",
            "1261390 [17/03 20:57:05]\n",
            "1261390 [17/03 20:57:05]\n",
            "1261390 [17/03 20:57:05]\n",
            "Training progress:  79% 5500/7000 [06:17<03:12,  7.80it/s, Loss=0.065558]1275803 [17/03 20:57:05]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:06]\n",
            "1275803 [17/03 20:57:07]\n",
            "Training progress:  79% 5510/7000 [06:18<03:13,  7.70it/s, Loss=0.065954]1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:07]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "Training progress:  79% 5520/7000 [06:20<03:12,  7.71it/s, Loss=0.072596]1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:08]\n",
            "1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:09]\n",
            "Training progress:  79% 5530/7000 [06:21<03:10,  7.70it/s, Loss=0.063372]1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:09]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "1275803 [17/03 20:57:10]\n",
            "Training progress:  79% 5540/7000 [06:22<03:09,  7.69it/s, Loss=0.067475]1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:11]\n",
            "1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:12]\n",
            "Training progress:  79% 5550/7000 [06:24<03:08,  7.69it/s, Loss=0.068895]1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:12]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "Training progress:  79% 5560/7000 [06:25<03:07,  7.70it/s, Loss=0.073705]1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:13]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:14]\n",
            "Training progress:  80% 5570/7000 [06:26<03:05,  7.70it/s, Loss=0.078409]1275803 [17/03 20:57:14]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:15]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "Training progress:  80% 5580/7000 [06:28<03:04,  7.70it/s, Loss=0.065512]1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:16]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "Training progress:  80% 5590/7000 [06:29<03:03,  7.70it/s, Loss=0.070472]1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:17]\n",
            "1275803 [17/03 20:57:18]\n",
            "1275803 [17/03 20:57:18]\n",
            "1275803 [17/03 20:57:18]\n",
            "1275803 [17/03 20:57:18]\n",
            "1275803 [17/03 20:57:18]\n",
            "1275803 [17/03 20:57:18]\n",
            "Training progress:  80% 5600/7000 [06:30<03:01,  7.72it/s, Loss=0.070444]1288419 [17/03 20:57:18]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:19]\n",
            "1288419 [17/03 20:57:20]\n",
            "Training progress:  80% 5610/7000 [06:32<03:02,  7.62it/s, Loss=0.079215]1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:20]\n",
            "1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:21]\n",
            "Training progress:  80% 5620/7000 [06:33<03:01,  7.62it/s, Loss=0.067097]1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:21]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "Training progress:  80% 5630/7000 [06:34<02:59,  7.63it/s, Loss=0.063123]1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:22]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "1288419 [17/03 20:57:23]\n",
            "Training progress:  81% 5640/7000 [06:35<02:58,  7.63it/s, Loss=0.064613]1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:24]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "Training progress:  81% 5650/7000 [06:37<02:56,  7.63it/s, Loss=0.072786]1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:25]\n",
            "1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:26]\n",
            "Training progress:  81% 5660/7000 [06:38<02:55,  7.63it/s, Loss=0.065030]1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:26]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "1288419 [17/03 20:57:27]\n",
            "Training progress:  81% 5670/7000 [06:39<02:54,  7.64it/s, Loss=0.065036]1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:28]\n",
            "1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:29]\n",
            "Training progress:  81% 5680/7000 [06:41<02:52,  7.64it/s, Loss=0.072479]1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:29]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "Training progress:  81% 5690/7000 [06:42<02:51,  7.65it/s, Loss=0.063718]1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:30]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "1288419 [17/03 20:57:31]\n",
            "Training progress:  81% 5700/7000 [06:43<02:50,  7.64it/s, Loss=0.065487]1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:32]\n",
            "1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:33]\n",
            "Training progress:  82% 5710/7000 [06:45<02:51,  7.54it/s, Loss=0.071097]1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:33]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "Training progress:  82% 5720/7000 [06:46<02:49,  7.55it/s, Loss=0.065885]1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:34]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:35]\n",
            "Training progress:  82% 5730/7000 [06:47<02:48,  7.55it/s, Loss=0.072496]1304132 [17/03 20:57:35]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:36]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "Training progress:  82% 5740/7000 [06:49<02:46,  7.55it/s, Loss=0.069048]1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:37]\n",
            "1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:38]\n",
            "Training progress:  82% 5750/7000 [06:50<02:45,  7.56it/s, Loss=0.063514]1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:38]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:39]\n",
            "Training progress:  82% 5760/7000 [06:51<02:44,  7.56it/s, Loss=0.062454]1304132 [17/03 20:57:39]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:40]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "Training progress:  82% 5770/7000 [06:53<02:42,  7.55it/s, Loss=0.073337]1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:41]\n",
            "1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:42]\n",
            "Training progress:  83% 5780/7000 [06:54<02:41,  7.54it/s, Loss=0.069934]1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:42]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:43]\n",
            "Training progress:  83% 5790/7000 [06:55<02:40,  7.54it/s, Loss=0.076666]1304132 [17/03 20:57:43]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:44]\n",
            "1304132 [17/03 20:57:45]\n",
            "1304132 [17/03 20:57:45]\n",
            "Training progress:  83% 5800/7000 [06:57<02:39,  7.54it/s, Loss=0.066379]1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:45]\n",
            "1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:46]\n",
            "Training progress:  83% 5810/7000 [06:58<02:39,  7.45it/s, Loss=0.067752]1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:46]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:47]\n",
            "Training progress:  83% 5820/7000 [06:59<02:38,  7.46it/s, Loss=0.068049]1317117 [17/03 20:57:47]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:48]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "Training progress:  83% 5830/7000 [07:01<02:36,  7.46it/s, Loss=0.073205]1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:49]\n",
            "1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:50]\n",
            "Training progress:  83% 5840/7000 [07:02<02:35,  7.47it/s, Loss=0.065952]1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:50]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:51]\n",
            "Training progress:  84% 5850/7000 [07:03<02:33,  7.47it/s, Loss=0.067341]1317117 [17/03 20:57:51]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:52]\n",
            "1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:53]\n",
            "Training progress:  84% 5860/7000 [07:05<02:32,  7.47it/s, Loss=0.089310]1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:53]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "Training progress:  84% 5870/7000 [07:06<02:31,  7.48it/s, Loss=0.071573]1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:54]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "1317117 [17/03 20:57:55]\n",
            "Training progress:  84% 5880/7000 [07:07<02:29,  7.48it/s, Loss=0.070486]1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:56]\n",
            "1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:57]\n",
            "Training progress:  84% 5890/7000 [07:09<02:28,  7.47it/s, Loss=0.077231]1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:57]\n",
            "1317117 [17/03 20:57:58]\n",
            "1317117 [17/03 20:57:58]\n",
            "1317117 [17/03 20:57:58]\n",
            "1317117 [17/03 20:57:58]\n",
            "1317117 [17/03 20:57:58]\n",
            "Training progress:  84% 5900/7000 [07:10<02:27,  7.47it/s, Loss=0.074612]1331043 [17/03 20:57:58]\n",
            "1331043 [17/03 20:57:58]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "1331043 [17/03 20:57:59]\n",
            "Training progress:  84% 5910/7000 [07:11<02:27,  7.37it/s, Loss=0.067870]1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:00]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "Training progress:  85% 5920/7000 [07:13<02:26,  7.39it/s, Loss=0.060324]1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:01]\n",
            "1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:02]\n",
            "Training progress:  85% 5930/7000 [07:14<02:24,  7.40it/s, Loss=0.070421]1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:02]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:03]\n",
            "1331043 [17/03 20:58:04]\n",
            "Training progress:  85% 5940/7000 [07:15<02:23,  7.40it/s, Loss=0.064597]1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:04]\n",
            "1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:05]\n",
            "Training progress:  85% 5950/7000 [07:17<02:21,  7.40it/s, Loss=0.059872]1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:05]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "Training progress:  85% 5960/7000 [07:18<02:20,  7.40it/s, Loss=0.074238]1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:06]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:07]\n",
            "1331043 [17/03 20:58:08]\n",
            "Training progress:  85% 5970/7000 [07:20<02:19,  7.40it/s, Loss=0.079383]1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:08]\n",
            "1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:09]\n",
            "Training progress:  85% 5980/7000 [07:21<02:17,  7.40it/s, Loss=0.066759]1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:09]\n",
            "1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:10]\n",
            "Training progress:  86% 5990/7000 [07:22<02:16,  7.39it/s, Loss=0.070724]1331043 [17/03 20:58:10]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:11]\n",
            "1331043 [17/03 20:58:12]\n",
            "Training progress:  86% 6000/7000 [07:24<02:15,  7.39it/s, Loss=0.074601]1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:12]\n",
            "1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:13]\n",
            "Training progress:  86% 6010/7000 [07:25<02:15,  7.31it/s, Loss=0.064569]1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:13]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "1345861 [17/03 20:58:14]\n",
            "Training progress:  86% 6020/7000 [07:26<02:14,  7.31it/s, Loss=0.072758]1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:15]\n",
            "1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:16]\n",
            "Training progress:  86% 6030/7000 [07:28<02:12,  7.31it/s, Loss=0.072963]1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:16]\n",
            "1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:17]\n",
            "Training progress:  86% 6040/7000 [07:29<02:11,  7.31it/s, Loss=0.069704]1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:17]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "1345861 [17/03 20:58:18]\n",
            "Training progress:  86% 6050/7000 [07:30<02:10,  7.30it/s, Loss=0.071691]1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:19]\n",
            "1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:20]\n",
            "Training progress:  87% 6060/7000 [07:32<02:08,  7.31it/s, Loss=0.061458]1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:20]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "Training progress:  87% 6070/7000 [07:33<02:07,  7.31it/s, Loss=0.064279]1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:21]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:22]\n",
            "1345861 [17/03 20:58:23]\n",
            "Training progress:  87% 6080/7000 [07:35<02:06,  7.30it/s, Loss=0.069316]1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:23]\n",
            "1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:24]\n",
            "Training progress:  87% 6090/7000 [07:36<02:04,  7.30it/s, Loss=0.067726]1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:24]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "1345861 [17/03 20:58:25]\n",
            "Training progress:  87% 6100/7000 [07:37<02:03,  7.31it/s, Loss=0.070783]1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:26]\n",
            "1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:27]\n",
            "Training progress:  87% 6110/7000 [07:39<02:03,  7.23it/s, Loss=0.080097]1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:27]\n",
            "1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:28]\n",
            "Training progress:  87% 6120/7000 [07:40<02:01,  7.24it/s, Loss=0.066498]1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:28]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:29]\n",
            "1356374 [17/03 20:58:30]\n",
            "Training progress:  88% 6130/7000 [07:41<02:00,  7.24it/s, Loss=0.077867]1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:30]\n",
            "1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:31]\n",
            "Training progress:  88% 6140/7000 [07:43<01:58,  7.25it/s, Loss=0.070802]1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:31]\n",
            "1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:32]\n",
            "Training progress:  88% 6150/7000 [07:44<01:56,  7.27it/s, Loss=0.059126]1356374 [17/03 20:58:32]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:33]\n",
            "1356374 [17/03 20:58:34]\n",
            "Training progress:  88% 6160/7000 [07:46<01:55,  7.28it/s, Loss=0.061903]1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:34]\n",
            "1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:35]\n",
            "Training progress:  88% 6170/7000 [07:47<01:54,  7.28it/s, Loss=0.060028]1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:35]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "1356374 [17/03 20:58:36]\n",
            "Training progress:  88% 6180/7000 [07:48<01:52,  7.28it/s, Loss=0.064579]1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:37]\n",
            "1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:38]\n",
            "Training progress:  88% 6190/7000 [07:50<01:51,  7.28it/s, Loss=0.068692]1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:38]\n",
            "1356374 [17/03 20:58:39]\n",
            "1356374 [17/03 20:58:39]\n",
            "1356374 [17/03 20:58:39]\n",
            "1356374 [17/03 20:58:39]\n",
            "1356374 [17/03 20:58:39]\n",
            "Training progress:  89% 6200/7000 [07:51<01:49,  7.28it/s, Loss=0.074707]1370384 [17/03 20:58:39]\n",
            "1370384 [17/03 20:58:39]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:40]\n",
            "1370384 [17/03 20:58:41]\n",
            "Training progress:  89% 6210/7000 [07:53<01:49,  7.19it/s, Loss=0.065116]1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:41]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "Training progress:  89% 6220/7000 [07:54<01:48,  7.19it/s, Loss=0.072790]1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:42]\n",
            "1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:43]\n",
            "Training progress:  89% 6230/7000 [07:55<01:46,  7.20it/s, Loss=0.070494]1370384 [17/03 20:58:43]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:44]\n",
            "1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:45]\n",
            "Training progress:  89% 6240/7000 [07:57<01:45,  7.19it/s, Loss=0.071083]1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:45]\n",
            "1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:46]\n",
            "Training progress:  89% 6250/7000 [07:58<01:44,  7.19it/s, Loss=0.068213]1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:46]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "1370384 [17/03 20:58:47]\n",
            "Training progress:  89% 6260/7000 [07:59<01:42,  7.19it/s, Loss=0.063961]1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:48]\n",
            "1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:49]\n",
            "Training progress:  90% 6270/7000 [08:01<01:41,  7.20it/s, Loss=0.060565]1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:49]\n",
            "1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:50]\n",
            "Training progress:  90% 6280/7000 [08:02<01:39,  7.21it/s, Loss=0.068011]1370384 [17/03 20:58:50]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:51]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "Training progress:  90% 6290/7000 [08:04<01:38,  7.21it/s, Loss=0.062023]1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:52]\n",
            "1370384 [17/03 20:58:53]\n",
            "1370384 [17/03 20:58:53]\n",
            "1370384 [17/03 20:58:53]\n",
            "1370384 [17/03 20:58:53]\n",
            "Training progress:  90% 6300/7000 [08:05<01:37,  7.21it/s, Loss=0.068731]1373056 [17/03 20:58:53]\n",
            "1373056 [17/03 20:58:53]\n",
            "1373056 [17/03 20:58:53]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "1373056 [17/03 20:58:54]\n",
            "Training progress:  90% 6310/7000 [08:06<01:36,  7.14it/s, Loss=0.073800]1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:55]\n",
            "1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:56]\n",
            "Training progress:  90% 6320/7000 [08:08<01:35,  7.15it/s, Loss=0.066109]1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:56]\n",
            "1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:57]\n",
            "Training progress:  90% 6330/7000 [08:09<01:33,  7.17it/s, Loss=0.069946]1373056 [17/03 20:58:57]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:58]\n",
            "1373056 [17/03 20:58:59]\n",
            "Training progress:  91% 6340/7000 [08:11<01:31,  7.18it/s, Loss=0.075552]1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:58:59]\n",
            "1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:00]\n",
            "Training progress:  91% 6350/7000 [08:12<01:30,  7.19it/s, Loss=0.068180]1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:00]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "1373056 [17/03 20:59:01]\n",
            "Training progress:  91% 6360/7000 [08:13<01:28,  7.20it/s, Loss=0.062364]1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:02]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "Training progress:  91% 6370/7000 [08:15<01:27,  7.20it/s, Loss=0.072093]1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:03]\n",
            "1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:04]\n",
            "Training progress:  91% 6380/7000 [08:16<01:26,  7.19it/s, Loss=0.067583]1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:04]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:05]\n",
            "1373056 [17/03 20:59:06]\n",
            "Training progress:  91% 6390/7000 [08:18<01:24,  7.20it/s, Loss=0.072565]1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:06]\n",
            "1373056 [17/03 20:59:07]\n",
            "1373056 [17/03 20:59:07]\n",
            "1373056 [17/03 20:59:07]\n",
            "1373056 [17/03 20:59:07]\n",
            "Training progress:  91% 6400/7000 [08:19<01:23,  7.20it/s, Loss=0.068265]1388813 [17/03 20:59:07]\n",
            "1388813 [17/03 20:59:07]\n",
            "1388813 [17/03 20:59:07]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "1388813 [17/03 20:59:08]\n",
            "Training progress:  92% 6410/7000 [08:20<01:23,  7.10it/s, Loss=0.065912]1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:09]\n",
            "1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:10]\n",
            "Training progress:  92% 6420/7000 [08:22<01:21,  7.10it/s, Loss=0.064078]1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:10]\n",
            "1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:11]\n",
            "Training progress:  92% 6430/7000 [08:23<01:20,  7.10it/s, Loss=0.065881]1388813 [17/03 20:59:11]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:12]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "Training progress:  92% 6440/7000 [08:25<01:18,  7.10it/s, Loss=0.062876]1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:13]\n",
            "1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:14]\n",
            "Training progress:  92% 6450/7000 [08:26<01:17,  7.10it/s, Loss=0.062963]1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:14]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "1388813 [17/03 20:59:15]\n",
            "Training progress:  92% 6460/7000 [08:27<01:16,  7.10it/s, Loss=0.066106]1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:16]\n",
            "1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:17]\n",
            "Training progress:  92% 6470/7000 [08:29<01:14,  7.10it/s, Loss=0.072264]1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:17]\n",
            "1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:18]\n",
            "Training progress:  93% 6480/7000 [08:30<01:13,  7.10it/s, Loss=0.066078]1388813 [17/03 20:59:18]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:19]\n",
            "1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:20]\n",
            "Training progress:  93% 6490/7000 [08:32<01:11,  7.10it/s, Loss=0.074547]1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:20]\n",
            "1388813 [17/03 20:59:21]\n",
            "1388813 [17/03 20:59:21]\n",
            "1388813 [17/03 20:59:21]\n",
            "1388813 [17/03 20:59:21]\n",
            "1388813 [17/03 20:59:21]\n",
            "Training progress:  93% 6500/7000 [08:33<01:10,  7.09it/s, Loss=0.064833]1397292 [17/03 20:59:21]\n",
            "1397292 [17/03 20:59:21]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:22]\n",
            "1397292 [17/03 20:59:23]\n",
            "Training progress:  93% 6510/7000 [08:35<01:09,  7.02it/s, Loss=0.073023]1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:23]\n",
            "1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:24]\n",
            "Training progress:  93% 6520/7000 [08:36<01:08,  7.04it/s, Loss=0.062835]1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:24]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "1397292 [17/03 20:59:25]\n",
            "Training progress:  93% 6530/7000 [08:37<01:06,  7.05it/s, Loss=0.059452]1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:26]\n",
            "1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:27]\n",
            "Training progress:  93% 6540/7000 [08:39<01:05,  7.05it/s, Loss=0.066990]1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:27]\n",
            "1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:28]\n",
            "Training progress:  94% 6550/7000 [08:40<01:03,  7.05it/s, Loss=0.066347]1397292 [17/03 20:59:28]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:29]\n",
            "1397292 [17/03 20:59:30]\n",
            "Training progress:  94% 6560/7000 [08:42<01:02,  7.06it/s, Loss=0.067454]1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:30]\n",
            "1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:31]\n",
            "Training progress:  94% 6570/7000 [08:43<01:00,  7.07it/s, Loss=0.059703]1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:31]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "1397292 [17/03 20:59:32]\n",
            "Training progress:  94% 6580/7000 [08:44<00:59,  7.08it/s, Loss=0.059736]1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:33]\n",
            "1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:34]\n",
            "Training progress:  94% 6590/7000 [08:46<00:57,  7.08it/s, Loss=0.066753]1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:34]\n",
            "1397292 [17/03 20:59:35]\n",
            "1397292 [17/03 20:59:35]\n",
            "1397292 [17/03 20:59:35]\n",
            "1397292 [17/03 20:59:35]\n",
            "1397292 [17/03 20:59:35]\n",
            "1397292 [17/03 20:59:35]\n",
            "Training progress:  94% 6600/7000 [08:47<00:56,  7.08it/s, Loss=0.066450]1408896 [17/03 20:59:35]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:36]\n",
            "1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:37]\n",
            "Training progress:  94% 6610/7000 [08:49<00:55,  6.99it/s, Loss=0.081596]1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:37]\n",
            "1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:38]\n",
            "Training progress:  95% 6620/7000 [08:50<00:54,  6.99it/s, Loss=0.061175]1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:38]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:39]\n",
            "1408896 [17/03 20:59:40]\n",
            "Training progress:  95% 6630/7000 [08:52<00:52,  7.00it/s, Loss=0.064814]1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:40]\n",
            "1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:41]\n",
            "Training progress:  95% 6640/7000 [08:53<00:51,  7.00it/s, Loss=0.077880]1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:41]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "1408896 [17/03 20:59:42]\n",
            "Training progress:  95% 6650/7000 [08:54<00:49,  7.01it/s, Loss=0.060042]1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:43]\n",
            "1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:44]\n",
            "Training progress:  95% 6660/7000 [08:56<00:48,  7.01it/s, Loss=0.058981]1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:44]\n",
            "1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:45]\n",
            "Training progress:  95% 6670/7000 [08:57<00:47,  7.00it/s, Loss=0.060983]1408896 [17/03 20:59:45]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:46]\n",
            "1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:47]\n",
            "Training progress:  95% 6680/7000 [08:59<00:45,  7.01it/s, Loss=0.064659]1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:47]\n",
            "1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:48]\n",
            "Training progress:  96% 6690/7000 [09:00<00:44,  7.01it/s, Loss=0.064705]1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:48]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:49]\n",
            "1408896 [17/03 20:59:50]\n",
            "Training progress:  96% 6700/7000 [09:02<00:42,  7.00it/s, Loss=0.058733]1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:50]\n",
            "1417800 [17/03 20:59:51]\n",
            "1417800 [17/03 20:59:51]\n",
            "1417800 [17/03 20:59:51]\n",
            "1417800 [17/03 20:59:51]\n",
            "Training progress:  96% 6710/7000 [09:03<00:41,  6.92it/s, Loss=0.063813]1417800 [17/03 20:59:51]\n",
            "1417800 [17/03 20:59:51]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:52]\n",
            "1417800 [17/03 20:59:53]\n",
            "Training progress:  96% 6720/7000 [09:04<00:40,  6.94it/s, Loss=0.065897]1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:53]\n",
            "1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:54]\n",
            "Training progress:  96% 6730/7000 [09:06<00:38,  6.95it/s, Loss=0.058715]1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:54]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "1417800 [17/03 20:59:55]\n",
            "Training progress:  96% 6740/7000 [09:07<00:37,  6.95it/s, Loss=0.066238]1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:56]\n",
            "1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:57]\n",
            "Training progress:  96% 6750/7000 [09:09<00:35,  6.96it/s, Loss=0.066570]1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:57]\n",
            "1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:58]\n",
            "Training progress:  97% 6760/7000 [09:10<00:34,  6.97it/s, Loss=0.066210]1417800 [17/03 20:59:58]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 20:59:59]\n",
            "1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:00]\n",
            "Training progress:  97% 6770/7000 [09:12<00:33,  6.96it/s, Loss=0.061087]1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:00]\n",
            "1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:01]\n",
            "Training progress:  97% 6780/7000 [09:13<00:31,  6.96it/s, Loss=0.056328]1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:01]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:02]\n",
            "1417800 [17/03 21:00:03]\n",
            "Training progress:  97% 6790/7000 [09:15<00:30,  6.97it/s, Loss=0.069005]1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:03]\n",
            "1417800 [17/03 21:00:04]\n",
            "1417800 [17/03 21:00:04]\n",
            "1417800 [17/03 21:00:04]\n",
            "1417800 [17/03 21:00:04]\n",
            "Training progress:  97% 6800/7000 [09:16<00:28,  6.97it/s, Loss=0.063554]1428600 [17/03 21:00:04]\n",
            "1428600 [17/03 21:00:04]\n",
            "1428600 [17/03 21:00:04]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "1428600 [17/03 21:00:05]\n",
            "Training progress:  97% 6810/7000 [09:17<00:27,  6.90it/s, Loss=0.067403]1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:06]\n",
            "1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:07]\n",
            "Training progress:  97% 6820/7000 [09:19<00:26,  6.91it/s, Loss=0.067940]1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:07]\n",
            "1428600 [17/03 21:00:08]\n",
            "1428600 [17/03 21:00:08]\n",
            "1428600 [17/03 21:00:08]\n",
            "1428600 [17/03 21:00:08]\n",
            "1428600 [17/03 21:00:08]\n",
            "1428600 [17/03 21:00:08]\n",
            "Training progress:  98% 6830/7000 [09:20<00:24,  6.91it/s, Loss=0.062570]1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:09]\n",
            "1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:10]\n",
            "Training progress:  98% 6840/7000 [09:22<00:23,  6.90it/s, Loss=0.059812]1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:10]\n",
            "1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:11]\n",
            "Training progress:  98% 6850/7000 [09:23<00:21,  6.90it/s, Loss=0.065292]1428600 [17/03 21:00:11]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:12]\n",
            "1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:13]\n",
            "Training progress:  98% 6860/7000 [09:25<00:20,  6.90it/s, Loss=0.072498]1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:13]\n",
            "1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:14]\n",
            "Training progress:  98% 6870/7000 [09:26<00:18,  6.91it/s, Loss=0.058604]1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:14]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:15]\n",
            "1428600 [17/03 21:00:16]\n",
            "Training progress:  98% 6880/7000 [09:28<00:17,  6.92it/s, Loss=0.056790]1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:16]\n",
            "1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:17]\n",
            "Training progress:  98% 6890/7000 [09:29<00:15,  6.92it/s, Loss=0.066537]1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:17]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "1428600 [17/03 21:00:18]\n",
            "Training progress:  99% 6900/7000 [09:30<00:14,  6.92it/s, Loss=0.062281]1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:19]\n",
            "1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:20]\n",
            "Training progress:  99% 6910/7000 [09:32<00:13,  6.85it/s, Loss=0.064308]1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:20]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "1438387 [17/03 21:00:21]\n",
            "Training progress:  99% 6920/7000 [09:33<00:11,  6.85it/s, Loss=0.064333]1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:22]\n",
            "1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:23]\n",
            "Training progress:  99% 6930/7000 [09:35<00:10,  6.85it/s, Loss=0.062996]1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:23]\n",
            "1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:24]\n",
            "Training progress:  99% 6940/7000 [09:36<00:08,  6.85it/s, Loss=0.070379]1438387 [17/03 21:00:24]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:25]\n",
            "1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:26]\n",
            "Training progress:  99% 6950/7000 [09:38<00:07,  6.86it/s, Loss=0.062258]1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:26]\n",
            "1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:27]\n",
            "Training progress:  99% 6960/7000 [09:39<00:05,  6.86it/s, Loss=0.059868]1438387 [17/03 21:00:27]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:28]\n",
            "1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:29]\n",
            "Training progress: 100% 6970/7000 [09:41<00:04,  6.86it/s, Loss=0.060654]1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:29]\n",
            "1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:30]\n",
            "Training progress: 100% 6980/7000 [09:42<00:02,  6.86it/s, Loss=0.080237]1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:30]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:31]\n",
            "1438387 [17/03 21:00:32]\n",
            "Training progress: 100% 6990/7000 [09:44<00:01,  6.87it/s, Loss=0.066841]1438387 [17/03 21:00:32]\n",
            "1438387 [17/03 21:00:32]\n",
            "1438387 [17/03 21:00:32]\n",
            "1438387 [17/03 21:00:32]\n",
            "1438387 [17/03 21:00:32]\n",
            "1438387 [17/03 21:00:33]\n",
            "1438387 [17/03 21:00:33]\n",
            "1438387 [17/03 21:00:33]\n",
            "1438387 [17/03 21:00:33]\n",
            "1438387 [17/03 21:00:33]\n",
            "Training progress: 100% 7000/7000 [09:45<00:00, 11.95it/s, Loss=0.069691]\n",
            "1438387 [17/03 21:00:33]\n",
            "1438387 [17/03 21:00:34]\n",
            "1438387 [17/03 21:00:35]\n",
            "1438387 [17/03 21:00:36]\n",
            "1438387 [17/03 21:00:36]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:37]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "\n",
            "[ITER 7000] Evaluating test: L1 0.04146947094704956 PSNR 23.401346683502197 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:38]\n",
            "1438387 [17/03 21:00:39]\n",
            "1438387 [17/03 21:00:40]\n",
            "1438387 [17/03 21:00:40]\n",
            "1438387 [17/03 21:00:41]\n",
            "\n",
            "[ITER 7000] Evaluating train: L1 0.036664871871471404 PSNR 24.456967926025392 [17/03 21:00:42]\n",
            "\n",
            "[ITER 7000] Saving Gaussians [17/03 21:00:42]\n",
            "\n",
            "Training complete. [17/03 21:00:55]\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/3dgs/gaussian-splatting/train.py -s /content/drive/MyDrive/3dgs/gaussian-splatting/tandt/truck/tank --eval --iteration 7000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEKBvwqyIFNK",
        "outputId": "9423918f-8e9a-42f0-f8fe-dbe3fa4ea216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for config file in /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1/cfg_args\n",
            "Config file found: /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1/cfg_args\n",
            "Rendering /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1\n",
            "Loading trained model at iteration 10 [02/02 01:35:23]\n",
            "------------LLFF HOLD------------- [02/02 01:35:24]\n",
            "Reading camera 185/185 [02/02 01:35:24]\n",
            "Loading Training Cameras [02/02 01:35:24]\n",
            "Loading Test Cameras [02/02 01:35:26]\n",
            "Rendering progress:   0% 0/161 [00:00<?, ?it/s](420, 648) [02/02 01:35:27]\n",
            "Rendering progress:   1% 1/161 [00:00<01:05,  2.43it/s](420, 648) [02/02 01:35:27]\n",
            "Rendering progress:   1% 2/161 [00:00<00:51,  3.09it/s](420, 648) [02/02 01:35:27]\n",
            "Rendering progress:   2% 3/161 [00:00<00:46,  3.37it/s](420, 648) [02/02 01:35:28]\n",
            "Rendering progress:   2% 4/161 [00:01<00:44,  3.52it/s](420, 648) [02/02 01:35:28]\n",
            "Rendering progress:   3% 5/161 [00:01<00:43,  3.62it/s](420, 648) [02/02 01:35:28]\n",
            "Rendering progress:   4% 6/161 [00:01<00:42,  3.65it/s](420, 648) [02/02 01:35:28]\n",
            "Rendering progress:   4% 7/161 [00:02<00:42,  3.67it/s](420, 648) [02/02 01:35:29]\n",
            "Rendering progress:   5% 8/161 [00:02<00:40,  3.76it/s](420, 648) [02/02 01:35:29]\n",
            "Rendering progress:   6% 9/161 [00:02<00:39,  3.81it/s](420, 648) [02/02 01:35:29]\n",
            "Rendering progress:   6% 10/161 [00:02<00:39,  3.81it/s](420, 648) [02/02 01:35:29]\n",
            "Rendering progress:   7% 11/161 [00:03<00:39,  3.82it/s](420, 648) [02/02 01:35:30]\n",
            "Rendering progress:   7% 12/161 [00:03<00:39,  3.75it/s](420, 648) [02/02 01:35:30]\n",
            "Rendering progress:   8% 13/161 [00:03<00:39,  3.75it/s](420, 648) [02/02 01:35:30]\n",
            "Rendering progress:   9% 14/161 [00:03<00:38,  3.81it/s](420, 648) [02/02 01:35:30]\n",
            "Rendering progress:   9% 15/161 [00:04<00:37,  3.85it/s](420, 648) [02/02 01:35:31]\n",
            "Rendering progress:  10% 16/161 [00:04<00:37,  3.89it/s](420, 648) [02/02 01:35:31]\n",
            "Rendering progress:  11% 17/161 [00:04<00:37,  3.84it/s](420, 648) [02/02 01:35:31]\n",
            "Rendering progress:  11% 18/161 [00:04<00:36,  3.87it/s](420, 648) [02/02 01:35:31]\n",
            "Rendering progress:  12% 19/161 [00:05<00:36,  3.85it/s](420, 648) [02/02 01:35:32]\n",
            "Rendering progress:  12% 20/161 [00:05<00:36,  3.85it/s](420, 648) [02/02 01:35:32]\n",
            "Rendering progress:  13% 21/161 [00:05<00:36,  3.85it/s](420, 648) [02/02 01:35:32]\n",
            "Rendering progress:  14% 22/161 [00:05<00:36,  3.84it/s](420, 648) [02/02 01:35:33]\n",
            "Rendering progress:  14% 23/161 [00:06<00:36,  3.74it/s](420, 648) [02/02 01:35:33]\n",
            "Rendering progress:  15% 24/161 [00:06<00:35,  3.81it/s](420, 648) [02/02 01:35:33]\n",
            "Rendering progress:  16% 25/161 [00:06<00:35,  3.87it/s](420, 648) [02/02 01:35:33]\n",
            "Rendering progress:  16% 26/161 [00:06<00:34,  3.88it/s](420, 648) [02/02 01:35:34]\n",
            "Rendering progress:  17% 27/161 [00:07<00:34,  3.87it/s](420, 648) [02/02 01:35:34]\n",
            "Rendering progress:  17% 28/161 [00:07<00:34,  3.87it/s](420, 648) [02/02 01:35:34]\n",
            "Rendering progress:  18% 29/161 [00:07<00:34,  3.84it/s](420, 648) [02/02 01:35:34]\n",
            "Rendering progress:  19% 30/161 [00:07<00:34,  3.78it/s](420, 648) [02/02 01:35:35]\n",
            "Rendering progress:  19% 31/161 [00:08<00:34,  3.74it/s](420, 648) [02/02 01:35:35]\n",
            "Rendering progress:  20% 32/161 [00:08<00:34,  3.74it/s](420, 648) [02/02 01:35:35]\n",
            "Rendering progress:  20% 33/161 [00:08<00:35,  3.64it/s](420, 648) [02/02 01:35:35]\n",
            "Rendering progress:  21% 34/161 [00:09<00:34,  3.64it/s](420, 648) [02/02 01:35:36]\n",
            "Rendering progress:  22% 35/161 [00:09<00:34,  3.64it/s](420, 648) [02/02 01:35:36]\n",
            "Rendering progress:  22% 36/161 [00:09<00:33,  3.69it/s](420, 648) [02/02 01:35:36]\n",
            "Rendering progress:  23% 37/161 [00:09<00:33,  3.75it/s](420, 648) [02/02 01:35:37]\n",
            "Rendering progress:  24% 38/161 [00:10<00:32,  3.80it/s](420, 648) [02/02 01:35:37]\n",
            "Rendering progress:  24% 39/161 [00:10<00:32,  3.74it/s](420, 648) [02/02 01:35:37]\n",
            "Rendering progress:  25% 40/161 [00:10<00:32,  3.77it/s](420, 648) [02/02 01:35:37]\n",
            "Rendering progress:  25% 41/161 [00:10<00:31,  3.78it/s](420, 648) [02/02 01:35:38]\n",
            "Rendering progress:  26% 42/161 [00:11<00:31,  3.78it/s](420, 648) [02/02 01:35:38]\n",
            "Rendering progress:  27% 43/161 [00:11<00:31,  3.80it/s](420, 648) [02/02 01:35:38]\n",
            "Rendering progress:  27% 44/161 [00:11<00:30,  3.82it/s](420, 648) [02/02 01:35:38]\n",
            "Rendering progress:  28% 45/161 [00:12<00:30,  3.82it/s](420, 648) [02/02 01:35:39]\n",
            "Rendering progress:  29% 46/161 [00:12<00:30,  3.81it/s](420, 648) [02/02 01:35:39]\n",
            "Rendering progress:  29% 47/161 [00:12<00:30,  3.78it/s](420, 648) [02/02 01:35:39]\n",
            "Rendering progress:  30% 48/161 [00:12<00:30,  3.77it/s](420, 648) [02/02 01:35:39]\n",
            "Rendering progress:  30% 49/161 [00:13<00:30,  3.72it/s](420, 648) [02/02 01:35:40]\n",
            "Rendering progress:  31% 50/161 [00:13<00:29,  3.71it/s](420, 648) [02/02 01:35:40]\n",
            "Rendering progress:  32% 51/161 [00:13<00:29,  3.70it/s](420, 648) [02/02 01:35:40]\n",
            "Rendering progress:  32% 52/161 [00:13<00:29,  3.71it/s](420, 648) [02/02 01:35:41]\n",
            "Rendering progress:  33% 53/161 [00:14<00:28,  3.73it/s](420, 648) [02/02 01:35:41]\n",
            "Rendering progress:  34% 54/161 [00:14<00:28,  3.75it/s](420, 648) [02/02 01:35:41]\n",
            "Rendering progress:  34% 55/161 [00:14<00:28,  3.75it/s](420, 648) [02/02 01:35:41]\n",
            "Rendering progress:  35% 56/161 [00:14<00:27,  3.82it/s](420, 648) [02/02 01:35:42]\n",
            "Rendering progress:  35% 57/161 [00:15<00:27,  3.80it/s](420, 648) [02/02 01:35:42]\n",
            "Rendering progress:  36% 58/161 [00:15<00:27,  3.75it/s](420, 648) [02/02 01:35:42]\n",
            "Rendering progress:  37% 59/161 [00:15<00:26,  3.82it/s](420, 648) [02/02 01:35:42]\n",
            "Rendering progress:  37% 60/161 [00:15<00:26,  3.85it/s](420, 648) [02/02 01:35:43]\n",
            "Rendering progress:  38% 61/161 [00:16<00:25,  3.90it/s](420, 648) [02/02 01:35:43]\n",
            "Rendering progress:  39% 62/161 [00:16<00:25,  3.93it/s](420, 648) [02/02 01:35:43]\n",
            "Rendering progress:  39% 63/161 [00:16<00:25,  3.87it/s](420, 648) [02/02 01:35:43]\n",
            "Rendering progress:  40% 64/161 [00:17<00:25,  3.81it/s](420, 648) [02/02 01:35:44]\n",
            "Rendering progress:  40% 65/161 [00:17<00:25,  3.74it/s](420, 648) [02/02 01:35:44]\n",
            "Rendering progress:  41% 66/161 [00:17<00:25,  3.73it/s](420, 648) [02/02 01:35:44]\n",
            "Rendering progress:  42% 67/161 [00:17<00:24,  3.77it/s](420, 648) [02/02 01:35:44]\n",
            "Rendering progress:  42% 68/161 [00:18<00:25,  3.72it/s](420, 648) [02/02 01:35:45]\n",
            "Rendering progress:  43% 69/161 [00:18<00:24,  3.71it/s](420, 648) [02/02 01:35:45]\n",
            "Rendering progress:  43% 70/161 [00:18<00:24,  3.68it/s](420, 648) [02/02 01:35:45]\n",
            "Rendering progress:  44% 71/161 [00:18<00:24,  3.69it/s](420, 648) [02/02 01:35:46]\n",
            "Rendering progress:  45% 72/161 [00:19<00:23,  3.74it/s](420, 648) [02/02 01:35:46]\n",
            "Rendering progress:  45% 73/161 [00:19<00:23,  3.68it/s](420, 648) [02/02 01:35:46]\n",
            "Rendering progress:  46% 74/161 [00:19<00:23,  3.71it/s](420, 648) [02/02 01:35:46]\n",
            "Rendering progress:  47% 75/161 [00:20<00:23,  3.69it/s](420, 648) [02/02 01:35:47]\n",
            "Rendering progress:  47% 76/161 [00:20<00:23,  3.66it/s](420, 648) [02/02 01:35:47]\n",
            "Rendering progress:  48% 77/161 [00:20<00:22,  3.65it/s](420, 648) [02/02 01:35:47]\n",
            "Rendering progress:  48% 78/161 [00:20<00:22,  3.63it/s](420, 648) [02/02 01:35:47]\n",
            "Rendering progress:  49% 79/161 [00:21<00:22,  3.65it/s](420, 648) [02/02 01:35:48]\n",
            "Rendering progress:  50% 80/161 [00:21<00:21,  3.71it/s](420, 648) [02/02 01:35:48]\n",
            "Rendering progress:  50% 81/161 [00:21<00:21,  3.76it/s](420, 648) [02/02 01:35:48]\n",
            "Rendering progress:  51% 82/161 [00:21<00:20,  3.78it/s](420, 648) [02/02 01:35:48]\n",
            "Rendering progress:  52% 83/161 [00:22<00:20,  3.82it/s](420, 648) [02/02 01:35:49]\n",
            "Rendering progress:  52% 84/161 [00:22<00:20,  3.85it/s](420, 648) [02/02 01:35:49]\n",
            "Rendering progress:  53% 85/161 [00:22<00:19,  3.87it/s](420, 648) [02/02 01:35:49]\n",
            "Rendering progress:  53% 86/161 [00:22<00:19,  3.89it/s](420, 648) [02/02 01:35:50]\n",
            "Rendering progress:  54% 87/161 [00:23<00:18,  3.91it/s](420, 648) [02/02 01:35:50]\n",
            "Rendering progress:  55% 88/161 [00:23<00:19,  3.83it/s](420, 648) [02/02 01:35:50]\n",
            "Rendering progress:  55% 89/161 [00:23<00:18,  3.81it/s](420, 648) [02/02 01:35:50]\n",
            "Rendering progress:  56% 90/161 [00:23<00:18,  3.79it/s](420, 648) [02/02 01:35:51]\n",
            "Rendering progress:  57% 91/161 [00:24<00:18,  3.73it/s](420, 648) [02/02 01:35:51]\n",
            "Rendering progress:  57% 92/161 [00:24<00:18,  3.71it/s](420, 648) [02/02 01:35:51]\n",
            "Rendering progress:  58% 93/161 [00:24<00:18,  3.66it/s](420, 648) [02/02 01:35:51]\n",
            "Rendering progress:  58% 94/161 [00:25<00:18,  3.70it/s](420, 648) [02/02 01:35:52]\n",
            "Rendering progress:  59% 95/161 [00:25<00:17,  3.72it/s](420, 648) [02/02 01:35:52]\n",
            "Rendering progress:  60% 96/161 [00:25<00:17,  3.71it/s](420, 648) [02/02 01:35:52]\n",
            "Rendering progress:  60% 97/161 [00:25<00:17,  3.72it/s](420, 648) [02/02 01:35:52]\n",
            "Rendering progress:  61% 98/161 [00:26<00:17,  3.65it/s](420, 648) [02/02 01:35:53]\n",
            "Rendering progress:  61% 99/161 [00:26<00:17,  3.63it/s](420, 648) [02/02 01:35:53]\n",
            "Rendering progress:  62% 100/161 [00:26<00:16,  3.63it/s](420, 648) [02/02 01:35:53]\n",
            "Rendering progress:  63% 101/161 [00:26<00:16,  3.61it/s](420, 648) [02/02 01:35:54]\n",
            "Rendering progress:  63% 102/161 [00:27<00:16,  3.58it/s](420, 648) [02/02 01:35:54]\n",
            "Rendering progress:  64% 103/161 [00:27<00:16,  3.56it/s](420, 648) [02/02 01:35:54]\n",
            "Rendering progress:  65% 104/161 [00:27<00:16,  3.56it/s](420, 648) [02/02 01:35:54]\n",
            "Rendering progress:  65% 105/161 [00:28<00:15,  3.58it/s](420, 648) [02/02 01:35:55]\n",
            "Rendering progress:  66% 106/161 [00:28<00:15,  3.65it/s](420, 648) [02/02 01:35:55]\n",
            "Rendering progress:  66% 107/161 [00:28<00:14,  3.64it/s](420, 648) [02/02 01:35:55]\n",
            "Rendering progress:  67% 108/161 [00:28<00:14,  3.70it/s](420, 648) [02/02 01:35:56]\n",
            "Rendering progress:  68% 109/161 [00:29<00:14,  3.70it/s](420, 648) [02/02 01:35:56]\n",
            "Rendering progress:  68% 110/161 [00:29<00:13,  3.78it/s](420, 648) [02/02 01:35:56]\n",
            "Rendering progress:  69% 111/161 [00:29<00:13,  3.81it/s](420, 648) [02/02 01:35:56]\n",
            "Rendering progress:  70% 112/161 [00:29<00:12,  3.78it/s](420, 648) [02/02 01:35:57]\n",
            "Rendering progress:  70% 113/161 [00:30<00:12,  3.78it/s](420, 648) [02/02 01:35:57]\n",
            "Rendering progress:  71% 114/161 [00:30<00:12,  3.80it/s](420, 648) [02/02 01:35:57]\n",
            "Rendering progress:  71% 115/161 [00:30<00:12,  3.73it/s](420, 648) [02/02 01:35:57]\n",
            "Rendering progress:  72% 116/161 [00:31<00:12,  3.67it/s](420, 648) [02/02 01:35:58]\n",
            "Rendering progress:  73% 117/161 [00:31<00:12,  3.59it/s](420, 648) [02/02 01:35:58]\n",
            "Rendering progress:  73% 118/161 [00:31<00:11,  3.63it/s](420, 648) [02/02 01:35:58]\n",
            "Rendering progress:  74% 119/161 [00:31<00:11,  3.70it/s](420, 648) [02/02 01:35:58]\n",
            "Rendering progress:  75% 120/161 [00:32<00:10,  3.73it/s](420, 648) [02/02 01:35:59]\n",
            "Rendering progress:  75% 121/161 [00:32<00:10,  3.67it/s](420, 648) [02/02 01:35:59]\n",
            "Rendering progress:  76% 122/161 [00:32<00:10,  3.63it/s](420, 648) [02/02 01:35:59]\n",
            "Rendering progress:  76% 123/161 [00:32<00:10,  3.65it/s](420, 648) [02/02 01:36:00]\n",
            "Rendering progress:  77% 124/161 [00:33<00:10,  3.65it/s](420, 648) [02/02 01:36:00]\n",
            "Rendering progress:  78% 125/161 [00:33<00:09,  3.63it/s](420, 648) [02/02 01:36:00]\n",
            "Rendering progress:  78% 126/161 [00:33<00:09,  3.61it/s](420, 648) [02/02 01:36:00]\n",
            "Rendering progress:  79% 127/161 [00:34<00:09,  3.52it/s](420, 648) [02/02 01:36:01]\n",
            "Rendering progress:  80% 128/161 [00:34<00:09,  3.55it/s](420, 648) [02/02 01:36:01]\n",
            "Rendering progress:  80% 129/161 [00:34<00:08,  3.60it/s](420, 648) [02/02 01:36:01]\n",
            "Rendering progress:  81% 130/161 [00:34<00:08,  3.64it/s](420, 648) [02/02 01:36:02]\n",
            "Rendering progress:  81% 131/161 [00:35<00:08,  3.66it/s](420, 648) [02/02 01:36:02]\n",
            "Rendering progress:  82% 132/161 [00:35<00:08,  3.61it/s](420, 648) [02/02 01:36:02]\n",
            "Rendering progress:  83% 133/161 [00:35<00:07,  3.63it/s](420, 648) [02/02 01:36:02]\n",
            "Rendering progress:  83% 134/161 [00:35<00:07,  3.67it/s](420, 648) [02/02 01:36:03]\n",
            "Rendering progress:  84% 135/161 [00:36<00:07,  3.66it/s](420, 648) [02/02 01:36:03]\n",
            "Rendering progress:  84% 136/161 [00:36<00:06,  3.71it/s](420, 648) [02/02 01:36:03]\n",
            "Rendering progress:  85% 137/161 [00:36<00:06,  3.72it/s](420, 648) [02/02 01:36:03]\n",
            "Rendering progress:  86% 138/161 [00:37<00:06,  3.76it/s](420, 648) [02/02 01:36:04]\n",
            "Rendering progress:  86% 139/161 [00:37<00:05,  3.78it/s](420, 648) [02/02 01:36:04]\n",
            "Rendering progress:  87% 140/161 [00:37<00:05,  3.68it/s](420, 648) [02/02 01:36:04]\n",
            "Rendering progress:  88% 141/161 [00:37<00:05,  3.65it/s](420, 648) [02/02 01:36:05]\n",
            "Rendering progress:  88% 142/161 [00:38<00:05,  3.65it/s](420, 648) [02/02 01:36:05]\n",
            "Rendering progress:  89% 143/161 [00:38<00:04,  3.62it/s](420, 648) [02/02 01:36:05]\n",
            "Rendering progress:  89% 144/161 [00:38<00:04,  3.60it/s](420, 648) [02/02 01:36:05]\n",
            "Rendering progress:  90% 145/161 [00:39<00:04,  3.57it/s](420, 648) [02/02 01:36:06]\n",
            "Rendering progress:  91% 146/161 [00:39<00:04,  3.58it/s](420, 648) [02/02 01:36:06]\n",
            "Rendering progress:  91% 147/161 [00:39<00:03,  3.62it/s](420, 648) [02/02 01:36:06]\n",
            "Rendering progress:  92% 148/161 [00:39<00:03,  3.63it/s](420, 648) [02/02 01:36:06]\n",
            "Rendering progress:  93% 149/161 [00:40<00:03,  3.51it/s](420, 648) [02/02 01:36:07]\n",
            "Rendering progress:  93% 150/161 [00:40<00:03,  3.51it/s](420, 648) [02/02 01:36:07]\n",
            "Rendering progress:  94% 151/161 [00:40<00:02,  3.51it/s](420, 648) [02/02 01:36:07]\n",
            "Rendering progress:  94% 152/161 [00:40<00:02,  3.53it/s](420, 648) [02/02 01:36:08]\n",
            "Rendering progress:  95% 153/161 [00:41<00:02,  3.48it/s](420, 648) [02/02 01:36:08]\n",
            "Rendering progress:  96% 154/161 [00:41<00:01,  3.55it/s](420, 648) [02/02 01:36:08]\n",
            "Rendering progress:  96% 155/161 [00:41<00:01,  3.62it/s](420, 648) [02/02 01:36:08]\n",
            "Rendering progress:  97% 156/161 [00:42<00:01,  3.67it/s](420, 648) [02/02 01:36:09]\n",
            "Rendering progress:  98% 157/161 [00:42<00:01,  3.71it/s](420, 648) [02/02 01:36:09]\n",
            "Rendering progress:  98% 158/161 [00:42<00:00,  3.69it/s](420, 648) [02/02 01:36:09]\n",
            "Rendering progress:  99% 159/161 [00:42<00:00,  3.70it/s](420, 648) [02/02 01:36:10]\n",
            "Rendering progress:  99% 160/161 [00:43<00:00,  3.73it/s](420, 648) [02/02 01:36:10]\n",
            "Rendering progress: 100% 161/161 [00:43<00:00,  3.71it/s]\n",
            "Rendering progress:   0% 0/24 [00:00<?, ?it/s](420, 648) [02/02 01:36:10]\n",
            "Rendering progress:   4% 1/24 [00:00<00:06,  3.83it/s](420, 648) [02/02 01:36:10]\n",
            "Rendering progress:   8% 2/24 [00:00<00:05,  3.84it/s](420, 648) [02/02 01:36:11]\n",
            "Rendering progress:  12% 3/24 [00:00<00:05,  3.90it/s](420, 648) [02/02 01:36:11]\n",
            "Rendering progress:  17% 4/24 [00:01<00:05,  3.85it/s](420, 648) [02/02 01:36:11]\n",
            "Rendering progress:  21% 5/24 [00:01<00:04,  3.93it/s](420, 648) [02/02 01:36:11]\n",
            "Rendering progress:  25% 6/24 [00:01<00:04,  3.84it/s](420, 648) [02/02 01:36:12]\n",
            "Rendering progress:  29% 7/24 [00:01<00:04,  3.74it/s](420, 648) [02/02 01:36:12]\n",
            "Rendering progress:  33% 8/24 [00:02<00:04,  3.73it/s](420, 648) [02/02 01:36:12]\n",
            "Rendering progress:  38% 9/24 [00:02<00:03,  3.81it/s](420, 648) [02/02 01:36:12]\n",
            "Rendering progress:  42% 10/24 [00:02<00:03,  3.79it/s](420, 648) [02/02 01:36:13]\n",
            "Rendering progress:  46% 11/24 [00:02<00:03,  3.81it/s](420, 648) [02/02 01:36:13]\n",
            "Rendering progress:  50% 12/24 [00:03<00:03,  3.72it/s](420, 648) [02/02 01:36:13]\n",
            "Rendering progress:  54% 13/24 [00:03<00:02,  3.77it/s](420, 648) [02/02 01:36:13]\n",
            "Rendering progress:  58% 14/24 [00:03<00:02,  3.73it/s](420, 648) [02/02 01:36:14]\n",
            "Rendering progress:  62% 15/24 [00:03<00:02,  3.71it/s](420, 648) [02/02 01:36:14]\n",
            "Rendering progress:  67% 16/24 [00:04<00:02,  3.70it/s](420, 648) [02/02 01:36:14]\n",
            "Rendering progress:  71% 17/24 [00:04<00:01,  3.78it/s](420, 648) [02/02 01:36:15]\n",
            "Rendering progress:  75% 18/24 [00:04<00:01,  3.73it/s](420, 648) [02/02 01:36:15]\n",
            "Rendering progress:  79% 19/24 [00:05<00:01,  3.64it/s](420, 648) [02/02 01:36:15]\n",
            "Rendering progress:  83% 20/24 [00:05<00:01,  3.68it/s](420, 648) [02/02 01:36:15]\n",
            "Rendering progress:  88% 21/24 [00:05<00:00,  3.67it/s](420, 648) [02/02 01:36:16]\n",
            "Rendering progress:  92% 22/24 [00:05<00:00,  3.64it/s](420, 648) [02/02 01:36:16]\n",
            "Rendering progress:  96% 23/24 [00:06<00:00,  3.62it/s](420, 648) [02/02 01:36:16]\n",
            "Rendering progress: 100% 24/24 [00:06<00:00,  3.74it/s]\n"
          ]
        }
      ],
      "source": [
        "!python render.py -m /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDrRThntIaRE",
        "outputId": "953ab125-ba19-496e-9ea8-4f87679a82d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scene: /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1\n",
            "Method: ours_10\n",
            "Metric evaluation progress: 100% 24/24 [00:36<00:00,  1.52s/it]\n",
            "  SSIM :    0.2898555\n",
            "  PSNR :   15.2216072\n",
            "  LPIPS:    0.7492551\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python metrics.py -m /content/drive/MyDrive/3dgs/gaussian-splatting/output/b22d6d7c-1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import torch\n",
        "def plot_gaussian_ellipsoid(mean, cov_matrix, ax, color='b', alpha=0.2):\n",
        "    \"\"\"\n",
        "    Plots a 3D Gaussian as an ellipsoid.\n",
        "    \"\"\"\n",
        "    from scipy.spatial.transform import Rotation as R\n",
        "    from scipy.linalg import sqrtm\n",
        "\n",
        "    U, S, Vt = torch.svd(cov_matrix.cpu())  # Decompose covariance\n",
        "    radii = torch.sqrt(S)  # Ellipsoid radii\n",
        "\n",
        "    # Create ellipsoid points\n",
        "    u = torch.linspace(0, 2 * torch.pi, 20)\n",
        "    v = torch.linspace(0, torch.pi, 10)\n",
        "    x = torch.outer(torch.cos(u), torch.sin(v))\n",
        "    y = torch.outer(torch.sin(u), torch.sin(v))\n",
        "    z = torch.outer(torch.ones_like(u), torch.cos(v))\n",
        "    xyz = torch.stack([x, y, z], dim=-1)\n",
        "\n",
        "    # Apply transformation\n",
        "    xyz = (xyz @ (Vt.T * radii).T).numpy()\n",
        "    x, y, z = xyz[..., 0] + mean[0].item(), xyz[..., 1] + mean[1].item(), xyz[..., 2] + mean[2].item()\n",
        "\n",
        "    ax.plot_wireframe(x, y, z, color=color, alpha=alpha)\n",
        "\n",
        "# Example usage\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for i in range(5):  # Plot first 5 Gaussians\n",
        "    mean = torch.rand(3)\n",
        "    cov = torch.eye(3) * (1 + i)  # Different scales\n",
        "    plot_gaussian_ellipsoid(mean, cov, ax)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "2SKB9x6HEJeE",
        "outputId": "fb56c396-18d7-4a9a-9c7a-2ef328bd72b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}